{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08040968",
   "metadata": {},
   "source": [
    "### <a name=\"index\"></a>Index\n",
    "\n",
    "[Algunos Conceptos Generales](#mark_01)\n",
    "\n",
    "[OLTP vs. OLAP](#mark_02)\n",
    "\n",
    "[Metodologías de Data Warehouse](#mark_03)\n",
    "\n",
    "   - [Bill Inmon](#mark_04)\n",
    "\n",
    "   - [Ralph Kimball](#mark_05)\n",
    "\n",
    "   - [Hefesto](#mark_06)\n",
    "\n",
    "[Data Warehouse - Data Lake - Data Lakehouse differencias](#mark_07)\n",
    "\n",
    "[Tipos de esquemas dimensionales](#mark_08)\n",
    "\n",
    "[Dimensiones lentamente cambiantes](#mark_09)\n",
    "\n",
    "   - [SCD tipo 1 \"Sobreescribir\"](#mark_10)\n",
    "\n",
    "   - [SCD tipo_2 \"Añadir fila\"](#mark_11)\n",
    "\n",
    "   - [SCD tipo_3 \"Añadir columna\"](#mark_12)\n",
    "\n",
    "   - [SCD tipo_4 \"Historial Separado\"](#mark_13)\n",
    "\n",
    "   - [SCD tipo_6 \"Híbrido\"](#mark_14)\n",
    "\n",
    "[Modelado dimensional: diseño de modelo](#mark_15)\n",
    "\n",
    "[Configuración de base de datos y herramientas para el flujo ETL.](#mark_16)\n",
    "\n",
    "[Documentación Dimensiones y Tabla Fact](#mark_17)\n",
    "\n",
    "<a name=\"index_01\"></a>\n",
    "\n",
    "[Documento de mapeo](#mark_18)\n",
    "\n",
    "[Creación del modelo físico](#mark_19)\n",
    "\n",
    "[Extracción: querys en SQL](#mark_20)\n",
    "\n",
    "[Extracción utilizando Pentaho](#mark_21)\n",
    "\n",
    "[Transformación: dimensión de cliente](#mark_22)\n",
    "\n",
    "[Carga: dimensión de cliente](#mark_23)\n",
    "\n",
    "[Soluciones ETL de las tablas de dimensiones y hechos](#mark_24)\n",
    "\n",
    "[Orquestación algunos parámetros extra](#mark_25)\n",
    "\n",
    "[Orquestar ETL en Pentaho: job](#mark_26)\n",
    "\n",
    "[Proyecto ETL completo](#mark_27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c2f867",
   "metadata": {},
   "source": [
    "# <a name=\"mark_01\"></a>Algunos Conceptos Generales: \n",
    "### [Index](#index)\n",
    "\n",
    "- Analítica Descriptiva: Examina datos históricos para entender eventos pasados y su impacto en el negocio.\n",
    "\n",
    "- Analítica Diagnóstica: Investiga las causas de eventos específicos, identificando patrones y relaciones.\n",
    "\n",
    "- Analítica Predictiva: Utiliza modelos estadísticos y algoritmos para preveer futuros eventos y tendencias.\n",
    "\n",
    "- Analítica Prescriptiva: Proporciona recomendaciones basadas en análisis previos para optimizar decisiones y acciones futuras.\n",
    "\n",
    "- Data Warehouse: Almacen\n",
    "\n",
    "- Data Mart: Áreas del Almacen\n",
    "\n",
    "- Dimensión: Análisis de una métrica desde distintas perspectivas\n",
    "\n",
    "- Hechos: Información cuantitativa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d941a5b",
   "metadata": {},
   "source": [
    "# <a name=\"mark_02\"></a>OLTP vs. OLAP:\n",
    "### [Index](#index)\n",
    "\n",
    "- OLTP (Procesamiento de Transacciones en Línea): \n",
    "    - Sistema enfocado en gestionar transacciones rápidas y eficientes, como ventas o pagos, en tiempo real.\n",
    "    - rapidez, eficiencia, operaciones en tiempo real.\n",
    "\n",
    "- OLAP (Procesamiento Analítico en Línea): \n",
    "    - Tecnología para analizar y consultar grandes volúmenes de datos multidimensionales, facilitando la toma de decisiones empresariales.\n",
    "    - análisis, consulta, datos multidimensionales, toma de decisiones.\n",
    "![](img_01.png)\n",
    "![](img_02.png)\n",
    "Nota: \"Data volatil\" se refiere a las modificaciones que pueden hacerce en un usuario, dirección, email, etc. La información es actualizada al momento sin historial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9009dab2",
   "metadata": {},
   "source": [
    "# <a name=\"mark_03\"></a>Metodologías de Data Warehouse:\n",
    "### [Index](#index)\n",
    "\n",
    "* Una métodología orientada a un Data Warehouse contiene básicamente 3 componentes:\n",
    "    - Un Source.\n",
    "    - Modelos de Dimensiones (Dimensional Models).\n",
    "    - Su Visualización.\n",
    "## Nota_01: Pueden cohexistir más de una metodología por proyecto.\n",
    "## Nota_02: Las Metodologías de DW no son fijas, pueden customizarce dependiendo las necesidades."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52388743",
   "metadata": {},
   "source": [
    "# <a name=\"mark_04\"></a>Bill Inmon:\n",
    "### [Index](#index)\n",
    "\n",
    "Precursor de las tecnologías de BI.\n",
    "\n",
    "1. En este modelo \"Staging\" es una DDBB temporal.\n",
    "    - Podemos hacer transformaciones muy pesadas en los datos sin afectar las DDBB del negocio.\n",
    "2. Proceso de ETL --> Data Warehouse.\n",
    "     \n",
    "![](img_03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8163df5e",
   "metadata": {},
   "source": [
    "# <a name=\"mark_05\"></a>Ralph Kimball:\n",
    "### [Index](#index)\n",
    "\n",
    "![](img_04.png)\n",
    "\n",
    "## Flujo para crear Modelo de Datos:\n",
    "![](img_05.png)\n",
    "\n",
    "Notas:\n",
    "- \"Planificación del Proyecto\" --> BI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219cd7d2",
   "metadata": {},
   "source": [
    "# <a name=\"mark_06\"></a>Hefesto:\n",
    "### [Index](#index)\n",
    "\n",
    "![](img_06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8424eb",
   "metadata": {},
   "source": [
    "# <a name=\"mark_07\"></a>Data Warehouse - Data Lake - Data Lakehouse differencias:\n",
    "### [Index](#index)\n",
    "\n",
    "* ¿Qué es un Data Lakehouse?\n",
    "\n",
    "Un Data Lakehouse es una arquitectura que combina la escalabilidad y rapidez de los Data Lakes con la eficiencia y confiabilidad de los Data Warehouses. \n",
    "\n",
    "Esta arquitectura se basa en un sistema de almacenamiento de datos distribuido, que permite la ingesta y procesamiento de grandes volúmenes de datos, y una capa de procesamiento que permite la transformación, integración y análisis de estos datos.\n",
    "\n",
    "A diferencia de los Data Lakes tradicionales, que se basan en almacenar datos sin procesar en su forma original, los Data Lakehouses organizan los datos en un esquema de tablas o columnas para mejorar la eficiencia en la consulta y en el análisis de datos. \n",
    "\n",
    "Por otro lado, a diferencia de los Data Warehouses tradicionales, los Data Lakehouses no requieren la definición previa de esquemas de datos y favorecen la escalabilidad horizontal, lo que facilita el procesamiento de grandes volúmenes de datos.\n",
    "\n",
    "* Diferencias entre un Data Warehouse, un Data Lake y un Data Lakehouse\n",
    "\n",
    "Resulta muy fácil confundir términos como Data Warehouse, Data Lake y Data Lakehouse. Cada uno de ellos se refiere a una arquitectura de almacenamiento de datos, pero tienen diferencias significativas.\n",
    "\n",
    "Un Data Warehouse es un repositorio centralizado de datos estructurados, orientado a las consultas y el análisis de negocios. \n",
    "![](img_07.png)\n",
    "La información se almacena en tablas relacionales y se estructura para facilitar la consulta y el análisis. \n",
    "Te recomiendo que leas el [artículo específico de Data Warehouse](https://aprenderbigdata.com/data-warehouse/) para tener más detalle.\n",
    "![](https://aprenderbigdata.com/wp-content/uploads/lakehouse-warehouse-datalake.png)\n",
    "Data Warehouse vs Data Lake vs Data Lakehouse [Databricks]\n",
    "\n",
    "Por otro lado, un Data Lake es un repositorio centralizado de datos sin estructurar y semiestructurados. \n",
    "![](img_08.png)\n",
    "Es una solución más flexible y escalable que permite el almacenamiento de datos en bruto sin procesar, lo que permite un análisis más exhaustivo y una exploración más profunda de los datos. \n",
    "\n",
    "Aquí tienes el [artículo en detalle sobre Data Lakes](https://aprenderbigdata.com/data-lake/).\n",
    "\n",
    "Un Data Lakehouse es una combinación de ambos. Es una solución híbrida que combina los beneficios de un Data Warehouse y de un Data Lake. \n",
    "\n",
    "Permite el almacenamiento y procesamiento de datos estructurados, semiestructurados y no estructurados, lo que significa que los datos pueden ser analizados y procesados según sea necesario. \n",
    "\n",
    "Además, los datos pueden ser utilizados tanto para analítica y para [machine learning](https://aprenderbigdata.com/machine-learning/).\n",
    "\n",
    "* Arquitectura de un Data Lakehouse\n",
    "\n",
    "     - La arquitectura de un Data Lakehouse por tanto combina elementos de un Data Warehouse y un Data Lake. \n",
    "\n",
    "     - Un Data Lakehouse se compone de un almacenamiento escalable y eficiente de datos en bruto, sin procesar. \n",
    "\n",
    "     - También, consta de una capa de procesamiento de datos flexible y escalable y de una capa de servicios para proporcionar un acceso controlado a los datos.\n",
    "\n",
    "**La capa de almacenamiento** contiene los datos en su formato nativo y están organizados por temas o dominios de negocio. \n",
    "\n",
    "El almacenamiento de datos se basa en una infraestructura de almacenamiento distribuida que puede escalar horizontalmente a medida que aumenta la cantidad de datos. Generalmente, serán servicios como ADLS en Azure y S3 en AWS.\n",
    "\n",
    "**La capa de procesamiento** de datos proporciona la capacidad de procesar grandes cantidades de datos de forma paralela y distribuida. \n",
    "\n",
    "Esta capa utiliza tecnologías como Apache Spark o Apache Flink para procesar los datos y transformarlos en información valiosa. \n",
    "\n",
    "**La capa de procesamiento de datos** se ejecuta en un clúster de procesamiento distribuido y escalable que puede ajustarse en función de la carga de trabajo, por ejemplo usando Databricks.\n",
    "\n",
    "**Por último, la capa de servicios** proporciona una capa de abstracción que permite a los usuarios acceder a los datos de forma segura y controlada. \n",
    "\n",
    "Esta capa incluye tecnologías como Apache Hive o Presto para permitir el acceso a los datos mediante SQL, y tecnologías de virtualización de datos para proporcionar acceso a los datos a través de APIs RESTful.\n",
    "\n",
    "* Ventajas de utilizar un Data Lakehouse en tu empresa\n",
    "\n",
    "Un Data Lakehouse ofrece varias ventajas técnicas y empresariales sobre las soluciones tradicionales de almacenamiento y procesamiento de datos. A continuación tienes un listado con algunas de las ventajas más importantes:\n",
    "\n",
    "- Flexibilidad: Al combinar características de un data lake y un data warehouse, ofrece una arquitectura altamente flexible. Puedes almacenar datos en su formato nativo sin tener que preocuparte por el esquema de datos, lo que facilita la integración de datos de diferentes fuentes. Además, puedes transformar, procesar y consultar los datos en tiempo real sin tener que moverlos a otra ubicación.\n",
    "- Escalabilidad: Un data lakehouse te permite escalar verticalmente y horizontalmente según tus necesidades. Puedes agregar más recursos de almacenamiento y procesamiento para manejar mayores volúmenes de datos y consultas más complejas. Además, puedes agregar nuevas fuentes de datos sin preocuparte por la capacidad de almacenamiento.\n",
    "- Eficiencia: Al evitar la necesidad de mover datos entre diferentes sistemas de almacenamiento y procesamiento, un data lakehouse puede reducir significativamente los tiempos de procesamiento y aumentar la eficiencia. También puedes utilizar motores de procesamiento distribuido, como Apache Spark, para procesar grandes volúmenes de datos en paralelo y acelerar el tiempo de procesamiento.\n",
    "- Coste: Al evitar la necesidad de mover datos entre diferentes sistemas de almacenamiento y procesamiento, un data lakehouse puede reducir significativamente el coste de almacenamiento y procesamiento.\n",
    "\n",
    "* Herramientas y tecnologías para implementar un Data Lakehouse\n",
    "\n",
    "Para implementar un Data Lakehouse, se necesita un conjunto de tecnologías y herramientas que permitan integrar, procesar y analizar los datos.\n",
    "\n",
    "Data Lakehouse IA\n",
    "Las opciones de almacenamiento para un Data Lakehouse pueden variar desde sistemas de almacenamiento de archivos distribuidos como HDFS o Amazon S3 hasta bases de datos columnares como Snowflake.\n",
    "\n",
    "Por otro lado, Spark es una tecnología ampliamente utilizada para procesamiento de datos en tiempo real y batch en un Data Lakehouse. \n",
    "\n",
    "Otras herramientas populares incluyen Apache Flink para streaming y Apache Beam.\n",
    "\n",
    "También necesitaremos herramientas de orquestación de flujo de trabajo e integración de datos. Algunas opciones son Apache Airflow, Apache Nifi y Apache Kafka.\n",
    "\n",
    "La elección de las herramientas y tecnologías adecuadas dependerá de las necesidades específicas de la empresa y de los datos que se estén procesando. \n",
    "\n",
    "Es muy importante conocer estas tecnologías para diseñar una solución adecuada y adaptada a estas necesidades.\n",
    "\n",
    "* Desafíos y consideraciones a tener en cuenta\n",
    "\n",
    "Aunque un Data Lakehouse puede ser una solución efectiva para manejar grandes volúmenes de datos y ofrecer una arquitectura más flexible y escalable, también hay ciertos desafíos y consideraciones importantes que debes tener en cuenta.\n",
    "\n",
    "Al igual que con cualquier solución de Big Data, es fundamental implementar medidas de seguridad adecuadas para proteger los datos sensibles y garantizar el cumplimiento normativo. \n",
    "\n",
    "Esto puede incluir la implementación de controles de acceso, encriptación y monitorización.\n",
    "\n",
    "Un Data Lakehouse puede generar grandes volúmenes de datos de diferentes fuentes y formatos. \n",
    "\n",
    "Es importante contar con herramientas y procesos para administrar y catalogar estos datos de manera efectiva. \n",
    "\n",
    "Debemos definir las políticas de datos, estandarizar los formatos usados y establecer mecanismos de housekeeping para eliminar datos obsoletos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358de9f",
   "metadata": {},
   "source": [
    "# <a name=\"mark_08\"></a>Tipos de esquemas dimensionales:\n",
    "### [Index](#index)\n",
    "\n",
    "![](img_09.png)\n",
    "\n",
    "![](img_10.png)\n",
    "\n",
    "![](img_11.png)\n",
    "\n",
    "![](img_12.png)\n",
    "\n",
    "![](img_13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b7bd07",
   "metadata": {},
   "source": [
    "# <a name=\"mark_09\"></a>Dimensiones lentamente cambiantes:\n",
    "### [Index](#index)\n",
    "\n",
    "![](img_14.png)\n",
    "\n",
    "![](img_15.png)\n",
    "\n",
    "![](img_16.png)\n",
    "\n",
    "![](img_17.png)\n",
    "\n",
    "Las Dimensiones lentamente cambiantes o SCD (Slowly Changing Dimensions) son Dimensiones en las cuales sus datos tienden a modificarse a través del tiempo, ya sea de forma ocasional o constante. \n",
    "\n",
    "Cuando ocurren estos cambios, se puede optar por seguir una de estas dos opciones:\n",
    "\n",
    "Registrar el historial de cambios.\n",
    "Reemplazar los valores que sean necesarios.\n",
    "Inicialmente Ralph Kimball planteó tres estrategias a seguir cuando se tratan las SCD: tipo 1, tipo 2 y tipo 3; pero a través de los años la comunidad de personas que se encargaba de modelar bases de datos profundizó las definiciones iniciales e incluyó varios tipos SCD más, por ejemplo: tipo 4 y tipo 6.\n",
    "\n",
    "A continuación se detallará cada tipo de estrategia SCD:\n",
    "\n",
    "- SCD Tipo 1: Sobreescribir.\n",
    "- SCD Tipo 2: Añadir fila.\n",
    "- SCD Tipo 3: Añadir columna.\n",
    "- SCD Tipo 4: Historial separado.\n",
    "- SCD Tipo 6: Híbrido.\n",
    "\n",
    "Cabe destacar que existe un SCD Tipo 0, que representa el NO tener en cuenta los cambios que pudieran llegar a suceder en los datos de las Dimensiones y por consiguiente NO tomar medidas.\n",
    "\n",
    "De acuerdo a la naturaleza del cambio se debe seleccionar qué técnica SCD se utilizará;  en algunos casos resultará conveniente combinar varias técnicas.\n",
    "\n",
    "Es importante señalar que si bien hay diferentes maneras de implementar cada técnica, es indispensable contar con claves subrogadas (id único) en las tablas de Dimensiones para  poder aplicar dichas técnicas.\n",
    "\n",
    "Al aplicar las diferentes técnicas SCD, en muchos casos se deberá modificar la estructura de la tabla de Dimensión con la que se esté trabajando, por lo cual estas modificaciones son recomendables hacerlas al momento de modelar la tabla; aunque también pueden hacerse una vez que ya se ha modelado y contiene datos;  así por ejemplo al añadir una nueva columna se deberán especificar los valores por defecto que adoptarán los registros de la tabla."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85253781",
   "metadata": {},
   "source": [
    "# <a name=\"mark_10\"></a>SCD tipo 1 \"Sobreescribir\":\n",
    "### [Index](#index)\n",
    "\n",
    "\n",
    "Este tipo es el más básico y sencillo de implementar, ya que si bien NO almacena los cambios históricos, tampoco requiere ningún modelado especial y NO necesita que se añadan nuevos registros a la tabla.\n",
    "\n",
    "En este caso cuando un registro presenta un cambio en alguno de los valores de sus campos, se debe proceder simplemente a actualizar el dato en cuestión, sobreescribiendo el antiguo.\n",
    "\n",
    "Para ejemplificar este caso, se tomará como referencia la siguiente tabla:\n",
    "\n",
    "![](img_18.png)\n",
    "\n",
    "Ahora, se supondrá que este producto ha cambiado de rubro, y ahora ha pasado a ser Rubro 2, entonces se obtendrá lo siguiente:\n",
    "\n",
    "![](img_19.png)\n",
    "        \n",
    "Usualmente este tipo es utilizado en casos en donde la información histórica no sea importante de mantener, tal como sucede cuando se debe modificar el valor de un registro porque tiene errores de ortografía.\n",
    "\n",
    "El ejemplo planteado es solo a fines prácticos, ya que con esta técnica, todos los movimientos realizados de Producto 1, que antes pertenecían al Rubro 1, ahora pasarán a ser del Rubro 2, lo cual creará una gran inconsistencia en el DW."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a9502",
   "metadata": {},
   "source": [
    "# <a name=\"mark_11\"></a>SCD tipo 2 \"Añadir fila\":\n",
    "### [Index](#index)\n",
    "\n",
    "Esta estrategia requiere que también se agreguen algunas columnas adicionales a la tabla de Dimensión, para que almacenen el historial de cambios.\n",
    "\n",
    "Las columnas que suelen agregarse son:\n",
    "\n",
    "    - fechaInicio: fecha desde que entró en vigencia el registro actual. Por defecto suele utilizarse una fecha muy antigua, ejemplo: 01/01/1000.\n",
    "    \n",
    "    - fechaFin: fecha en la cual el registro actual dejó de estar en vigencia. Por defecto suele utilizarse una fecha muy futurista, ejemplo: 01/01/9999.\n",
    "\n",
    "    - version: número secuencial que se incrementa cada nuevo cambio. Por defecto suele comenzar en 1.\n",
    "    \n",
    "    - versionActual: especifica si el campo actual es el vigente. Este valor puede ser en caso de ser verdadero: true o 1; y en caso de ser falso: false o 0.\n",
    "    \n",
    "Entonces, cuando ocurra algún cambio en los valores de los registros, se añadirá una nueva fila y se deberán completar los datos referidos al historial de cambios.\n",
    "\n",
    "Para ejemplificar este caso, se tomará como referencia la siguiente tabla:\n",
    "\n",
    "![](img_20.png)\n",
    "A continuación se añadirán las columnas que almacenarán el historial:\n",
    "\n",
    "![](img_21.png)\n",
    "Ahora, se supondrá que este producto ha cambiado de Rubro, y ahora a pasado a ser Rubro 2, entonces se obtendrá lo siguiente:\n",
    "\n",
    "![](img_22.png)\n",
    "Como puede observarse, se lleva a cabo el siguiente proceso:\n",
    "\n",
    "    - Se añade una nueva fila con su correspondiente clave subrogada (idProducto).\n",
    "\n",
    "    - Se registra la modificación (rubro).\n",
    "\n",
    "    - Se actualizan los valores de fechaInicio y fechaFin, tanto de la fila nueva, como la antigua (la que presentó el cambio).\n",
    "\n",
    "    - Se incrementa en uno el valor del campo version que posee la fila antigua.\n",
    "\n",
    "    - Se actualizan los valores de versionActual, tanto de la fila nueva, como la antigua; dejando a la fila nueva como el registro vigente (true).\n",
    "    \n",
    "Esta técnica permite guardar ilimitada información de cambios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bd09fc",
   "metadata": {},
   "source": [
    "# <a name=\"mark_12\"></a>SCD tipo 3 \"Añadir columna\":\n",
    "### [Index](#index)\n",
    "\n",
    "Esta estrategia requiere que se agregue a la tabla de Dimensión una columna adicional por cada columna cuyos valores se desean mantener en un historial de cambios.\n",
    "\n",
    "![](img_20.png)\n",
    "Para mantener el histórico de cambios sobre los datos de la columna rubro se añadirá la columna rubroAnterior:\n",
    "\n",
    "![](img_23.png)\n",
    "Ahora, se supondrá que este producto ha cambiado de rubro, y ahora ha pasado a ser Rubro 2, entonces se obtendrá lo siguiente:\n",
    "\n",
    "![](img_24.png)\n",
    "Como puede observarse, se lleva a cabo el siguiente proceso:\n",
    "\n",
    "    - En la columna rubroAnterior se coloca el valor antiguo.\n",
    "    - En la columna rubro se coloca el nuevo valor vigente.\n",
    "Esta técnica permite guardar una limitada información de cambios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9278c7",
   "metadata": {},
   "source": [
    "# <a name=\"mark_13\"></a>SCD tipo 4 \"Historial separado\":\n",
    "### [Index](#index)\n",
    "\n",
    "\n",
    "Esta técnica se utiliza en combinación con alguna otra y su función básica es almacenar en una tabla adicional los detalles de cambios históricos realizados en una tabla de Dimensión.\n",
    "\n",
    "Esta tabla histórica indicará por ejemplo qué tipo de operación se ha realizado (Insert, Update, Delete), sobre qué campo y en qué fecha.\n",
    "\n",
    "El objetivo de mantener esta tabla es el de contar con un detalle de todos los cambios, para luego analizarlos y poder tomar decisiones acerca de cuál técnica SCD podría aplicarse mejor.\n",
    "\n",
    "Por ejemplo, la siguiente tabla histórica registra los cambios de la tabla de Dimensión dimProductos, la cual supondremos emplea el SCD Tipo 2:\n",
    "\n",
    "![](img_25.png)\n",
    "\n",
    "Tomando como ejemplo el primer registro de esta tabla, la información allí guardada indica lo siguiente:\n",
    "\n",
    "    - El día 05/06/2000, el registro de la tabla de Dimensión dimProductos con idProducto igual a 1 sufrió un cambio de rubro, por lo cual se debió insertar (Insert) una nueva fila con los valores vigentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d399e56d",
   "metadata": {},
   "source": [
    "# <a name=\"mark_14\"></a>SCD tipo 6 \"Híbrido\":\n",
    "### [Index](#index)\n",
    "\n",
    "El SCD Tipo 6 se basa en combinar diferentes técnicas SCD, ellas son:\n",
    "\n",
    "- SCD Tipo 1,\n",
    "- SCD Tipo 2 y\n",
    "- SCD Tipo 3.\n",
    "\n",
    "Y se denomina SCD Tipo 6, simplemente porque:\n",
    "\n",
    "6 = 1 + 2 +3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703f2fd9",
   "metadata": {},
   "source": [
    "# <a name=\"mark_15\"></a>Modelado dimensional: diseño de modelo\n",
    "### [Index](#index)\n",
    "\n",
    "Utilizando https://dbdiagram.io/d para el modelado de diseño generamos el siguiente modelo.\n",
    "\n",
    "```sql\n",
    "// Use DBML to define your database structure\n",
    "// Docs: https://dbml.dbdiagram.io/docs\n",
    "\n",
    "table dws.dim_clientes {\n",
    "  id_cliente int pk\n",
    "  codigo_cliente varchar  \n",
    "  nombre varchar\n",
    "  apellido varchar\n",
    "  nombre_apellido varchar\n",
    "  numero_celular varchar\n",
    "  numero_casa varchar\n",
    "  numero_trabajo varchar\n",
    "  ciudad_casa varchar\n",
    "}\n",
    "Ref: dws.dim_clientes.id_cliente < dws.fact_ventas.id_cliente\n",
    "\n",
    "table dws.dim_productos{\n",
    "  id_producto int pk\n",
    "  codigo_producto varchar\n",
    "  nombre_producto varchar\n",
    "  color varchar\n",
    "  tamanio varchar\n",
    "  categoria varchar\n",
    "}\n",
    "Ref: dws.dim_productos.id_producto < dws.fact_ventas.id_producto\n",
    "\n",
    "table dws.fact_ventas{\n",
    "  id_venta int pk\n",
    "  id_cliente int pk\n",
    "  id_producto int pk\n",
    "  cantidad int\n",
    "  valor decimal\n",
    "  descuento decimal\n",
    "  valor_neto decimal\n",
    "}\n",
    "\n",
    "```\n",
    "![](img_26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3be88b5",
   "metadata": {},
   "source": [
    "# <a name=\"mark_16\"></a>Configuración de base de datos y herramientas para el flujo ETL.\n",
    "### [Index](#index)\n",
    "Fuente: https://platzi.com/clases/7034-data-warehouse/61567-configuracion-de-setup-para-data-warehouse-y-etl/\n",
    "\n",
    "¡Hola, te doy la bienvenida a este tutorial! Configurarás las bases de datos y herramientas que usaremos para el ETL y crear un data warehouse.\n",
    "\n",
    "Usaremos PostgreSQL con la base de datos Adventureworks. Será nuestra base de datos transaccional y la fuente de información para llevar al data warehouse.\n",
    "\n",
    "Ejecuta las siguientes instrucciones para configurar esto:\n",
    "\n",
    "Ruby\n",
    "\n",
    "Instalación de Ruby en Ubuntu o WSL con Ubuntu\n",
    "1. Abre la terminal de Ubuntu\n",
    "2. Ejecuta el siguiente comando en la terminal para actualizar la lista de paquetes disponibles:\n",
    "```sh\n",
    "sudo apt-get update\n",
    "```\n",
    "3. Una vez actualizada la lista de paquetes, instala Ruby ejecutando el siguiente comando en la terminal:\n",
    "```sh\n",
    "sudo apt-get install ruby-full\n",
    "```\n",
    "4. Verifica que Ruby se haya instalado correctamente ejecutando ruby -v en la terminal.\n",
    "\n",
    "Instalación de Ruby en Windows\n",
    "\n",
    "1. Descarga el instalador de Ruby desde la página oficial de Ruby para Windows: https://rubyinstaller.org/downloads/\n",
    "2. Selecciona la versión de Ruby que deseas instalar.\n",
    "3. Ejecuta el instalador y sigue las instrucciones del asistente de instalación.\n",
    "4. Una vez completada la instalación, abre la línea de comandos de Windows (cmd.exe) y escribe ruby -v para verificar que la instalación se haya realizado correctamente.\n",
    "\n",
    "Instalación de Ruby en macOS\n",
    "\n",
    "1. Abre la terminal de macOS.\n",
    "2. Instala Homebrew ejecutando el siguiente comando en la terminal:\n",
    "\n",
    "/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n",
    "\n",
    "3. Una vez instalado Homebrew, ejecuta el siguiente comando en la terminal para instalar Ruby\n",
    "```sh\n",
    "brew install ruby\n",
    "```\n",
    "4. Verifica que Ruby se haya instalado correctamente ejecutando ruby -v en la terminal.\n",
    "Con estos pasos ya has instalado Ruby.\n",
    "\n",
    "PostgreSQL y pgAdmin o DBeaver:\n",
    "\n",
    "Estas herramientas ya deberías tenerla instaladas. Si no las tienes, vuelve a revisar esta clase tutorial o sigue la documentación de PostgreSQL. ⬅️💡\n",
    "\n",
    "⚠️Nota: si usas Windows recuerda asignar las variables de entorno para PostgreSQL.\n",
    "\n",
    "![](img_27.png)\n",
    "\n",
    "Descarga y configuración de la base de datos AdventureWorks\n",
    "\n",
    "1. Descarga el repositorio en https://github.com/lorint/AdventureWorks-for-Postgres\n",
    "\n",
    "Ejecuta el siguiente comando de Git:\n",
    "\n",
    "git clone https://github.com/lorint/AdventureWorks-for-Postgres.git\n",
    "\n",
    "Este repositorio contiene los archivos para crear las tablas y vistas de la base de datos.\n",
    "\n",
    "2. Descarga Adventure Works 2014 OLTP Script.\n",
    "\n",
    "Contiene los archivos para llenar las tablas de la base de datos.\n",
    "\n",
    "3. Copia y pega el archivo AdventureWorks-oltp-install-script.zip en el directorio AdventureWorks-for-Postgres.\n",
    "\n",
    "4. En tu terminal úbicate en el directorio AdventureWorks-for-Postgres y descomprime AdventureWorks-oltp-install-script.zip:\n",
    "```sh\n",
    "cd AdventureWorks-for-Postgres/\n",
    "unzip AdventureWorks-oltp-install-script.zip\n",
    "```\n",
    "5. En la terminal, ubicándote en el directorio AdventureWorks-for-Postgres, ejecuta el siguiente comando para convertir los archivos csv:\n",
    "```sh\n",
    "ruby update_csvs.rb\n",
    "```\n",
    "6. Activa la conexión con postgresql:\n",
    "```sh\n",
    "sudo service postgresql start\n",
    "```\n",
    "7. Crea la base de datos con el siguiente comando de PostgreSQL:\n",
    "```sh\n",
    "psql -c \"CREATE DATABASE \\\"Adventureworks\\\";\"\n",
    "```\n",
    "o\n",
    "```sh\n",
    "psql -c \"CREATE DATABASE \\\"Adventureworks\\\";\" -U postgres -h localhost\n",
    "```\n",
    "8. Ejecuta el script que llena las tablas de la base de datos:\n",
    "```sh\n",
    "psql -d Adventureworks < install.sql\n",
    "```\n",
    "o\n",
    "```sh\n",
    "psql -d Adventureworks < install.sql -U postgres -h localhost\n",
    "```\n",
    "# Nota: En este punto hay que hacer una modificación en el archivo insatll.sql para indicarle donde tiene que ir a buscar los datos para cada archivo CSV, en cada FROM hay que agregar el path correspondiente del archivo, de la siguiente forma.\n",
    "```sql\n",
    "\\copy HumanResources.EmployeeDepartmentHistory FROM 'C:/Users/mende/OneDrive/Documents/Data_Warehousing_y_Modelado_OLAP_Project/AdventureWorks-for-Postgres/EmployeeDepartmentHistory.csv' DELIMITER E'\\t' CSV;\n",
    "SELECT 'Copying data into HumanResources.EmployeePayHistory';\n",
    "```\n",
    "# Luego se ejecuta desde PgAdmn PSQL Tool la siguiente instrucción:\n",
    "```sh\n",
    "\\i C:/Users/mende/OneDrive/Documents/Data_Warehousing_y_Modelado_OLAP_Project/AdventureWorks-for-Postgres/install\n",
    "_02.sql\n",
    "```\n",
    "```\n",
    "# Donde \\i ejecuta archivos. \n",
    "\n",
    "#Para ver los help ejecutar \"\\?\"\n",
    "```\n",
    "9. Conecta tu base de datos en DBeaver o pgAdmin.\n",
    "\n",
    "    1. Abre DBeaver o pgAdmin.\n",
    "\n",
    "    2. Selecciona la opción para crear una nueva conexión.\n",
    "\n",
    "    3. Selecciona PostgreSQL en la lista de bases de datos.\n",
    "\n",
    "    4. Ingresa la información de conexión necesaria en la pestaña.\n",
    "\n",
    "        - Host: localhost\n",
    "        - Port: 5432\n",
    "        - Base de datos: Adventureworks\n",
    "        - Nombre de usuario: postgres\n",
    "        - Password: la que tengas de tu user de postgresql.\n",
    "![](img_28.png)\n",
    "\n",
    "5. Haz clic en **Test Connection** para asegurarte de que los detalles de conexión sean correctos y que puedas conectarte a la base de datos.\n",
    "6. Si la prueba de conexión es exitosa, haz clic en \"Finalizar\" para guardar la configuración de la conexión.\n",
    "\n",
    "Configuración de Pentaho\n",
    "\n",
    "Esta herramienta la utilizaremos para crear las ETL de los datos transaccionales (DB Adventureworks) en Postgres a el Data Warehouse en AWS Redshift.\n",
    "\n",
    "Esta herramienta deberías tenerla instalada del Curso de Fundamentos de ETL con Python y Pentaho. Si no la tienes revisa esta clase tutorial. ⬅️💡\n",
    "\n",
    "Instalación y configuración de AWS CLI\n",
    "\n",
    "Este servicio lo usarás para realizar la conexión a S3 y cargar archivos planos que luego serán cargados a AWS Redshift con el comando COPY.\n",
    "\n",
    "Esta herramienta la configuraste en el Curso Práctico de AWS: Roles y Seguridad con IAM en su módulo SDK, CLI y AWS Access Keys. ⬅️💡\n",
    "\n",
    "Vuelve a ver esas clases o sigue la siguiente documentación de AWS si no lo tienes configurado:\n",
    "\n",
    "Instalar AWS CLI: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html\n",
    "\n",
    "Configurar AWS CLI: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html\n",
    "\n",
    "Configuración de AWS Redshift\n",
    "\n",
    "AWS Redshift será utilizado como data warehouse. Será el lugar donde construiremos las dimensiones, tablas de hechos y llevaremos los datos modelados y limpios que se obtuvieron del sistema transaccional.\n",
    "\n",
    "1. Crea un nuevo clúster de AWS Redshift de manera similar al Curso de Fundamentos de ETL con Python y Pentaho. Puedes seguir las clases tutoriales de ese curso:\n",
    "\n",
    "    - Configuración de clúster en AWS Redshift.\n",
    "    \n",
    "⚠️ Recuerda nombrar diferente al clúster de AWS Redshift y al bucket de AWS S3 que usarás para el proyecto de este curso.\n",
    "\n",
    "Con esto has completado la configuración de herramientas a usar en las siguientes clases del curso.\n",
    "\n",
    "Deja en los comentarios si tienes alguna duda o problema que impida tu progreso, para que en comunidad podamos apoyarte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52763669",
   "metadata": {},
   "source": [
    "# <a name=\"mark_17\"></a>Análisis Dimensiones y Tabla Fact.\n",
    "### [Index](#index)\n",
    "\n",
    "# Importante: La generación del modelado ETL ![](img_26.png) es independiente de donde y como se extraen sus datos, a continuación se realiza el análisis para la extracción de cada uno de esas columnas de sus tablas correspondientes.\n",
    "\n",
    "Análisis \"dim_clientes\":\n",
    "\n",
    "\n",
    "\n",
    "```sql\n",
    "\n",
    "--Ejecutar luego de realizar la primera carga de datos en las dimensiones con Pentaho!!!!\n",
    "\n",
    "INSERT INTO dwh_adventureworks.dim_clientes\n",
    "(id_cliente, codigo_cliente, nombre, apellido, nombre_completo, numero_telefono_celular, numero_telefono_casa, numero_telefono_trabajo, ciudad_casa, fecha_carga, fecha_actualizacion)\n",
    "VALUES(-1, '-1', 'Sin Información', 'Sin Información', 'Sin Información', '', '', '', '', '1900/01/01 00:00:00', '1900/01/01 00:00:00');\n",
    "\n",
    "\n",
    "INSERT INTO dwh_adventureworks.dim_productos\n",
    "(id_producto, codigo_producto, nombre, color, tamanio, categoria, fecha_carga, fecha_actualizacion)\n",
    "VALUES(-1, '-1', 'Sin Información', '', '', '', '1900/01/01 00:00:00', '1900/01/01 00:00:00');\n",
    "\n",
    "\n",
    "INSERT INTO dwh_adventureworks.dim_territorios\n",
    "(id_territorio, codigo_territorio, nombre, continente, fecha_carga, fecha_actualizacion)\n",
    "VALUES(-1, '-1', 'Sin Información', '', '1900/01/01 00:00:00', '1900/01/01 00:00:00');\n",
    "\n",
    "\n",
    "INSERT INTO dwh_adventureworks.dim_vendedores\n",
    "(id_vendedor, codigo_vendedor, identificación, nombre, apellido, nombre_completo, rol, fecha_nacimiento, genero, ind_activo, fecha_inicio, fecha_fin, version, fecha_carga)\n",
    "VALUES(-1, '-1', null, 'Sin Información', 'Sin Información', 'Sin Información', null, '1900/01/01 00:00:00', null, true, '1900/01/01 00:00:00', '9999/12/31 00:00:00', 1, '1900/01/01 00:00:00');\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b55659",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d0a3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key, value name, {'first': 'Robert', 'middle': '', 'last': 'Smith'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': {'first': 'Robert', 'last': 'Smith'},\n",
       " 'age': 25,\n",
       " 'education': {'highschool': 'N/A', 'college': 'Yale'}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_file= {\n",
    "    'name':\n",
    "   {\n",
    "       'first': 'Robert', \n",
    "       'middle': '', \n",
    "       'last': 'Smith'\n",
    "   }, \n",
    "   'age': 25, \n",
    "   'DOB': '-', \n",
    "   'hobbies': ['running', 'coding', '-'], \n",
    "   'education': \n",
    "   {\n",
    "       'highschool': 'N/A', \n",
    "       'college': 'Yale'\n",
    "   }\n",
    "  }\n",
    "def first_test (**kwargs):\n",
    "    for key,value in kwargs.items():\n",
    "    \n",
    "        if type(value) == dict:\n",
    "            new_dict = value\n",
    "            print(f\"key, value {key}, {value}\")\n",
    "            for new_key, new_value in new_dict.items():\n",
    "                if new_value == \"-\" or new_value == None or new_value == \"\":\n",
    "                    key_to_delete = new_key\n",
    "                \n",
    "            del kwargs[key][key_to_delete]\n",
    "        \n",
    "        kwargs_copy_01 = kwargs.copy()\n",
    "            \n",
    "        for sec_key, sec_value in kwargs_copy_01.items():\n",
    "            if sec_value == \"-\" or sec_value == \"N/A\" or sec_value == \"\":\n",
    "                sec_key = sec_key\n",
    "                break\n",
    "        del kwargs_copy_01[sec_key]\n",
    "        \n",
    "        kwargs_copy_02 = kwargs_copy_01.copy()\n",
    "        \n",
    "        for key,value in kwargs_copy_02.items():\n",
    "            if type(value) == list and ('-' in value or '' in value or 'N/A'):\n",
    "                third_key_to_delete = key\n",
    "        del kwargs_copy_02[third_key_to_delete]\n",
    "            \n",
    "            \n",
    "        return kwargs_copy_02\n",
    "            \n",
    "    \n",
    "\n",
    "first_test(**dict_file)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d591422",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_file= {\n",
    "    'name':\n",
    "   {\n",
    "       'first': 'Robert', \n",
    "       'middle': '', \n",
    "       'last': 'Smith'\n",
    "   }, \n",
    "   'age': 25, \n",
    "   'DOB': '-', \n",
    "   'hobbies': ['running', 'coding', '-'], \n",
    "   'education': \n",
    "   {\n",
    "       'highschool': 'N/A', \n",
    "       'college': 'Yale'\n",
    "   }\n",
    "  }\n",
    "def first_test (**kwargs):\n",
    "    for key,value in kwargs.items():\n",
    "        \n",
    "        if value == \"-\" or sec_value == \"N/A\" or sec_value == \"\":\n",
    "                sec_key = sec_key\n",
    "        \n",
    "        del kwargs_copy_01[sec_key]\n",
    "        \n",
    "        if type(value) == dict:\n",
    "            new_dict = value\n",
    "            print(f\"key, value {key}, {value}\")\n",
    "            for new_key, new_value in new_dict.items():\n",
    "                if new_value == \"-\" or new_value == None or new_value == \"\":\n",
    "                    key_to_delete = new_key\n",
    "                \n",
    "            del kwargs[key][key_to_delete]\n",
    "        \n",
    "        kwargs_copy_01 = kwargs.copy()\n",
    "            \n",
    "        '''for sec_key, sec_value in kwargs_copy_01.items():\n",
    "            if sec_value == \"-\" or sec_value == \"N/A\" or sec_value == \"\":\n",
    "                sec_key = sec_key\n",
    "                break\n",
    "        del kwargs_copy_01[sec_key]'''\n",
    "        \n",
    "        kwargs_copy_02 = kwargs_copy_01.copy()\n",
    "        \n",
    "        for key,value in kwargs_copy_02.items():\n",
    "            if type(value) == list and ('-' in value or '' in value or 'N/A'):\n",
    "                third_key_to_delete = key\n",
    "        del kwargs_copy_02[third_key_to_delete]\n",
    "            \n",
    "            \n",
    "        return kwargs_copy_02\n",
    "            \n",
    "    \n",
    "\n",
    "first_test(**dict_file)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae6a056",
   "metadata": {},
   "source": [
    "### <a name=\"mark_18\"></a>Documento de mapeo\n",
    "\n",
    "### [Index](#index_01)\n",
    "\n",
    "Luego de realizar el modelo dimensional que reflejaremos en nuestro DW, documentaremos desde que fuente obtenedromos los datos para cada dimensión.\n",
    "\n",
    "Siendo nuestro modelo dimensional.\n",
    "\n",
    "![](img_29.png)\n",
    "\n",
    "Ahora debemos saber desde donde llenaremos estas tablas, comenzaremos por la **dim_clientes**. Inicialmente exploramos la tabla **customer**, desde donde podemos obtener el nuestro primer valor **codigo_cliente**\n",
    "\n",
    "![](img_31.png)\n",
    "\n",
    "![](img_32.png)\n",
    "\n",
    "Cómo necesitamos los demás valores debemos inferir en que tablas se encuentra para eso realizamos una exploración el diagrama de la tabla, en DBviewer es de la siguiente forma.\n",
    "\n",
    "![](img_30.png)\n",
    "\n",
    "Tal cual vemos en esta imágen, los valores datos de la persona se podrían sacar de la tabla **person**, no lo dice, pero hay un eschema **person** con lo cual la búsqueda es `select * from person.person`\n",
    "\n",
    "![](img_33.png)\n",
    "\n",
    "Finalmente el nuestro documento de mapeo para la **dim_clientes** quedará de la siguiente forma.\n",
    "\n",
    "![](img_34.png)\n",
    "\n",
    "### Nota!: Este análisis debe ser realizado con cada dimensión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ad7a22",
   "metadata": {},
   "source": [
    "### <a name=\"mark_19\"></a>Creación del modelo físico\n",
    "\n",
    "### [Index](#index_01)\n",
    "\n",
    "Habiendo realizado el análisis para nuestras tablas dimensionales y tabla de hechos, no dirigimos a nuestro DW para crearlas.\n",
    "\n",
    "Para este ejemplo se muestra una DDBB en el DW Redshift, ahí construiremos las tablas dimensionales y tabla de hechos.\n",
    "\n",
    "![](img_35.png)\n",
    "\n",
    "la cual debe quedar la de siguiente forma:\n",
    "\n",
    "![](img_36.png)\n",
    "\n",
    "A continuación el código para crear las tablas:\n",
    "\n",
    "```sql\n",
    "--drop table dim_clientes;\n",
    "create table if not exists dim_clientes \n",
    "(\n",
    "\tid_cliente\tinteger\n",
    "\t,codigo_cliente\tvarchar(20)\n",
    "\t,nombre\tvarchar(50)\n",
    "\t,apellido\tvarchar(50)\n",
    "\t,nombre_completo\tvarchar(100)\n",
    "\t,numero_telefono_celular\tvarchar(20)\n",
    "\t,numero_telefono_casa\tvarchar(20)\n",
    "\t,numero_telefono_trabajo\tvarchar(20)\n",
    "\t,ciudad_casa\tvarchar(50)\n",
    "\t,fecha_carga timestamp\n",
    "\t,fecha_actualizacion timestamp\n",
    "\t,primary key (id_cliente)\n",
    ") \n",
    ";\n",
    "\n",
    "\n",
    "--drop table dim_productos;\n",
    "create table if not exists dim_productos \n",
    "(\n",
    "\tid_producto\tinteger\n",
    "\t,codigo_producto\tvarchar(20)\n",
    "\t,nombre\tvarchar(50)\n",
    "\t,color\tvarchar(50)\n",
    "\t,tamanio\tvarchar(50)\n",
    "\t,categoria\tvarchar(50)\n",
    "\t,fecha_carga timestamp\n",
    "\t,fecha_actualizacion timestamp\n",
    "\t,primary key (id_producto)\n",
    ") \n",
    ";\n",
    "\n",
    "\n",
    "--drop table dim_territorios;\n",
    "create table if not exists dim_territorios\n",
    "(\n",
    "\tid_territorio\tinteger\n",
    "\t,codigo_territorio\tvarchar(20)\n",
    "\t,nombre\tvarchar(50)\n",
    "\t,continente\tvarchar(50)\n",
    "\t,fecha_carga timestamp\n",
    "\t,fecha_actualizacion timestamp\n",
    "\t,primary key (id_territorio)\n",
    ") \n",
    ";\n",
    "\n",
    "\n",
    "--drop table dim_vendedores;\n",
    "create table if not exists dim_vendedores \n",
    "(\n",
    "\tid_vendedor\tinteger\n",
    "\t,codigo_vendedor\tvarchar(20)\n",
    "\t,identificación\tvarchar(20)\n",
    "\t,nombre\tvarchar(50)\n",
    "\t,apellido\tvarchar(50)\n",
    "\t,nombre_completo\tvarchar(50)\n",
    "\t,rol\tvarchar(50)\n",
    "\t,fecha_nacimiento\tdate\n",
    "\t,genero\tvarchar(10)\n",
    "\t,ind_activo\tboolean\n",
    "\t,fecha_inicio\tdate\n",
    "\t,fecha_fin\tdate\n",
    "\t,version integer\n",
    "\t,fecha_carga timestamp\n",
    "\t,primary key (id_vendedor)\n",
    ") \n",
    ";\n",
    "\n",
    "\n",
    "--drop table fact_ventas;\n",
    "CREATE TABLE if not exists fact_ventas (\n",
    "\tid_venta integer NOT NULL,\n",
    "\tcodigo_venta_detalle varchar(10) NOT NULL,\n",
    "\tcodigo_venta_encabezado varchar(10) NOT NULL,\n",
    "\tid_fecha integer NULL,\n",
    "\tid_territorio integer NULL,\n",
    "\tid_cliente integer NULL,\n",
    "\tid_vendedor integer NULL,\n",
    "\tid_producto integer NULL,\n",
    "\tcantidad integer NULL,\n",
    "\tvalor numeric(18,2) NULL,\n",
    "\tdescuento numeric(18,2) NULL,\n",
    "\tfecha_carga timestamp NULL,\n",
    "\tfecha_actualizacion timestamp NULL,\n",
    "\tCONSTRAINT fact_ventas_pkey PRIMARY KEY (id_venta)\n",
    ")\n",
    "\n",
    "\n",
    "--drop table dim_tiempo;\n",
    "--esta tabla fue creada para agregar un nivel mas de granularidad, esta tabla también se cargará desde el ETL.\n",
    "create table if not exists dim_tiempo\n",
    "(\n",
    "    id_fecha int not null,\n",
    "    fecha date not null, \n",
    "    dia smallint not null,\n",
    "    mes smallint not null,\n",
    "    anio smallint not null,\n",
    "    dia_semana smallint not null,\n",
    "    dia_anio smallint not null,\n",
    "\tPRIMARY KEY (id_fecha)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e09ad9",
   "metadata": {},
   "source": [
    "### <a name=\"mark_20\"></a>Extracción: querys en SQL\n",
    "\n",
    "### [Index](#index_01)\n",
    "\n",
    "El objetivo es tomar los datos desde nuestra base de datos **Adventureworks** y llevarla a nuestro DW en Redshift.\n",
    "\n",
    "A continuación las diferentes queries que traen los datos para llenas las tablas de dimensiones y tabla de hechos.\n",
    "\n",
    "### QUERY --> dim_cliente \n",
    "\n",
    "```sql\n",
    "select \n",
    "c.customerid as codigo_cliente,\n",
    "p.firstname as nombre,\n",
    "p.lastname as apellido,\n",
    "p.firstname || ' ' ||p.lastname as nombre_apellido,\n",
    "case when ph.phonenumbertypeid = 1 then ph.phonenumber else null end as numero_celular,\n",
    "case when ph.phonenumbertypeid = 2 then ph.phonenumber else null end as numero_casa,\n",
    "case when ph.phonenumbertypeid = 3 then ph.phonenumber else null end as numero_trabajo,\n",
    "ps.name as ciudad_casa\n",
    "\n",
    "from sales.customer as c\n",
    "inner join person.person p\n",
    "on c.personid = p.businessentityid\n",
    "left join person.personphone as ph\n",
    "on ph.businessentityid = c.personid\n",
    "left join person.stateprovince as ps\n",
    "on ps.stateprovinceid = c.territoryid\n",
    "\n",
    "```\n",
    "\n",
    "### QUERY --> dim_productos\n",
    "```sql\n",
    "select \n",
    "pp.productnumber as codigo_producto \n",
    ",pp.name as nombre_producto\n",
    ",pp.color as color\n",
    ",pp.size as tamanio\n",
    ",ppc.name as categoria\n",
    "from production.product as pp\n",
    "\n",
    "left join production.productcategory as ppc\n",
    "on ppc.productcategoryid = pp.productid\n",
    "\n",
    "```\n",
    "\n",
    "### QUERY --> dim_vendedores\n",
    "```sql\n",
    "select \n",
    "ssp.businessentityid as codigo_vendedor \n",
    ",hre.nationalidnumber as identificacion\n",
    ",pp.firstname as nombre \n",
    ",pp.lastname as apellido\n",
    ",pp.firstname || ' ' ||pp.lastname as nombre_completo\n",
    ",hre.jobtitle as rol\n",
    ",hre.birthdate as fecha_nacimiento\n",
    ",case \n",
    "\twhen hre.gender = 'M' then 'Masculino'\n",
    "\twhen hre.gender = 'F' then 'Femenino'\n",
    "end as genero\n",
    ",hre.currentflag as ind_activo\n",
    ",case \n",
    "\twhen ssp.bonus = 0 then 'sin bono' else 'tiene bono'\n",
    "end as ind_bono\n",
    "from sales.salesperson ssp\n",
    "\n",
    "left join humanresources.employee as hre\n",
    "on hre.businessentityid = ssp.businessentityid\n",
    "left join person.person as pp\n",
    "on pp.businessentityid = ssp.businessentityid\n",
    "```\n",
    "\n",
    "### QUERY --> dim_territorio\n",
    "```sql\n",
    "--dim_territorio\n",
    "select \n",
    "territoryid as codigo_territorio \n",
    ",name as nombre \n",
    ",case\n",
    "\twhen countryregioncode = 'US' or countryregioncode = 'CA' then 'North America'\n",
    "\twhen countryregioncode = 'FR' or countryregioncode = 'DE' or countryregioncode = 'GB' then 'Europe'\n",
    "\twhen countryregioncode = 'AU' then 'Pacifico'\n",
    "end as continente\n",
    "from sales.salesterritory\n",
    "```\n",
    "\n",
    "### QUERY --> fact_ventas\n",
    "```sql\n",
    "-- fact_ventas\n",
    "select\n",
    "orderqty as cantidad\n",
    ",orderqty * unitprice as valor\n",
    ",unitpricediscount * orderqty as descuento \n",
    ",(orderqty * unitprice - unitpricediscount * orderqty) as valor_neto\n",
    "from sales.salesorderdetail\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae802a",
   "metadata": {},
   "source": [
    "### <a name=\"mark_21\"></a>Extracción utilizando Pentaho\n",
    "\n",
    "### [Index](#index_01)\n",
    "\n",
    "### Pentaho es comparable a Glue, DataFactory, Data Services, Integration Services\n",
    "\n",
    "En Pentaho creamos una nueva transformación:\n",
    "\n",
    "![](img_37.png)\n",
    "\n",
    "Creamos el primer paso que tendrá un input desde una ddbb:\n",
    "\n",
    "![](img_38.png)\n",
    "\n",
    "![](img_39.png)\n",
    "\n",
    "![](img_40.png)\n",
    "\n",
    "![](img_41.png)\n",
    "\n",
    "![](img_42.png)\n",
    "\n",
    "A continuación vemos que se han creado dos tipos de input.\n",
    "\n",
    "* input_clientes: trae todos los datos de la base de datos transaccional en Postgres.\n",
    "\n",
    "* input_dim_clientes: trea todos los datos des de la dimensión clientes en el DW de Redshift.\n",
    "\n",
    "El objetivo de estos es identificar los nuevos datos que ingresan a **input_dim_clientes**, o sea, si ya existen quedan con su viejo id, y si son nuevos se les crea un nuevo id.\n",
    "\n",
    "En el **input_clientes** tenemos todos los datos extraidos mediantes query de nuestro source en Postgres con todas sus columnas, pero para **input_dim_clientes** (Table input 2) solo seleccionaremos algunas columnas. Esta selección se puede realizar mediante código o utilizando el botón \"Get SQL select statement\", como se indica en la imange.\n",
    "\n",
    "![](img_47.png)\n",
    "\n",
    "![](img_48.png)\n",
    "\n",
    "![](img_49.png)\n",
    "\n",
    "![](img_50.png)\n",
    "\n",
    "Como vimos solo son importantes el \"id_cliente\" para saber si el id ya existe o es nuevo, el \"codigo_cliente\" y \"fecha_actualizacion\" para conocer su última modificación.\n",
    "\n",
    "![](img_51.png)\n",
    "\n",
    "Renombramos los inputs:\n",
    "\n",
    "![](img_43.png)\n",
    "\n",
    "Para realizar este análisis relacionamos los dos inputs con un **Stream look-up** (buscar flujo) para identificar los registros nuevos de los viejos.\n",
    "\n",
    "![](img_44.png)\n",
    "\n",
    "Relacionamos los dos inputs en un Stream look-up\n",
    "\n",
    "![](img_45.png)\n",
    "\n",
    "Aplicamos la lógica:\n",
    "\n",
    "    _ Buscar en el step: \"input_dim_clientes\", la fuente de datos.\n",
    "    _ La llave para buscar los valores: \"cod_cliente\" que viene desde \"input_clientes\", y \"codigo_cliente\" que viene desde \"input_dim_clientes\" \n",
    "\n",
    "![](img_46.png)\n",
    "\n",
    "![](img_52.png)\n",
    "\n",
    "![](img_53.png)\n",
    "\n",
    "En realidad el cambo \"id_cliente\" NO EXISTE en ninguna input, supongo que si no existe lo crea y le otorga \"-1\" si no tiene valor. \n",
    "\n",
    "Haste este punto obtivimos los datos de la base transaccional y los datos de la base dimensional, los cruzaremos para obtener los registros nuevos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffce15a",
   "metadata": {},
   "source": [
    "### <a name=\"mark_22\"></a>Transformación: dimensión de cliente\n",
    "\n",
    "### [Index](#index_01)\n",
    "\n",
    "El siguiente paso es crear un paso para obtener la fecha del sistema, esta será la fecha en que se está ejecutando la ingesta.\n",
    "\n",
    "Utilizamos **Get System Info**:\n",
    "\n",
    "![](img_54.png)\n",
    "\n",
    "![](img_55.png)\n",
    "\n",
    "![](img_56.png)\n",
    "\n",
    "![](img_57.png)\n",
    "\n",
    "Si ejecutamos con un preview hasta esta parte del ETL:\n",
    "\n",
    "![](img_58.png)\n",
    "\n",
    "![](img_59.png)\n",
    "\n",
    "![](img_60.png)\n",
    "\n",
    "Ahora creamos una constante, la cual utilizaremos más adelante, este campo nuevo servirá para hacer una copia del id y lo utilizaremos para identificar que registros son nuevos (-1) y cuales son viejos (los que no tienen un -1), con lo cual nos ayudará a decidir si tenemos que insertar un nuevo registro o debemos realizar un update en nuestra tabla dimensional.\n",
    "\n",
    "![](img_61.png)\n",
    "\n",
    "![](img_62.png)\n",
    "\n",
    "Luego le asignaremos el valor de \"id_cliente\" a esa constante creada, que es la variable que está trajendo \"-1\" o distitno a \"-1\".\n",
    "\n",
    "![](img_63.png)\n",
    "\n",
    "![](img_64.png)\n",
    "\n",
    "El siguiente paso es validar si debemos crear o no el id, o sea si el \"id_cliente\" es \"-1\".\n",
    "\n",
    "![](img_65.png)\n",
    "\n",
    "![](img_66.png)\n",
    "\n",
    "![](img_67.png)\n",
    "\n",
    "Realizamos el flujo de asignarle el mismo id que tiene, o sea el id ya existia en la tabla dimensional. Creamos una nueva constante.\n",
    "\n",
    "![](img_68.png)\n",
    "\n",
    "![](img_69.png)\n",
    "\n",
    "![](img_70.png)\n",
    "\n",
    "![](img_71.png)\n",
    "\n",
    "A continuación vemos que el seteo para \"Add sequences\" necesita que el valor con el que inicia el conteo no sea fijo sino una variable ya que en el futuro existiran nuevos valores y estos incrementarán el valor inicial, con lo cual Max_registro irá cambiando y tomando diferentes valores en el tiempo, al momento de realizar la previw del ETL esta variable se puede modificar desde el botón \"config\".\n",
    "\n",
    "![](img_72.png)\n",
    "\n",
    "Unimos los dos flujos en un Dummy:\n",
    "\n",
    "![](img_73.png)\n",
    "\n",
    "Ahora creamos la formula que me diga que cuando el valor de Max_Cliente_id sea -1 hay que asignarle un nuevo id, y si es diferenete a -1 asigne el mismo id existente.\n",
    "\n",
    "![](img_74.png)\n",
    "\n",
    "![](img_75.png)\n",
    "\n",
    "![](img_76.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ad30c9",
   "metadata": {},
   "source": [
    "### <a name=\"mark_23\"></a>Carga: dimensión de cliente\n",
    "\n",
    "### [Index](#index_01)\n",
    "\n",
    "Ahora vamos a validar, que registros va a ir para un insert y que registros van a ir para un update. \n",
    "\n",
    "![](img_78.png)\n",
    "\n",
    "Entonces si es un registro nuevo, debido a que Redshift no tiene buena performance realizando inserciones o updtes, primero extraemos los datos en un csv.\n",
    "\n",
    "Pero si el registro ya existe y solo hay que cambiarlo, como la modificación es menor, podemos hacer un update directamente, el botón \"Browser\" de \"Target Table\" buscamos la tabla dentro del schema.\n",
    "\n",
    "Si el `id_cliente = id_cliente`, el primer \"id_cliente\" pertenece a los datos que vienen por el pipeline, y el segundo pertenece al \"id_cliente\" que se encuentra en la tabla dimensional de Redshift.\n",
    "\n",
    "Luego, con el botón \"Get Update Fields\", obtenemos la lista de campos disponibles de la tabla dimensional en Redshift (\"Table fields\") y en los que vienen por el flujo (\"Stream fields\"), hay que revisar uno por uno que los nombres sean coherentes con sus fuentes. Tener en cuenta podemos encontrarnos en el caso de no tener que realizar un update a ciertas columnas, que deberán ser eliminadas de esa lista.\n",
    "\n",
    "![](img_79.png)\n",
    "\n",
    "Cuando cumpla la condición de que sean registros nuevos el output sera un archivo csv que irá hacia un bucket de S3.\n",
    "\n",
    "En extension, seleccionar \"csv\".\n",
    "\n",
    "![](img_80.png)\n",
    "\n",
    "Colocamos el tipo de separados y quitamos el encabezado para que no genere problemas.\n",
    "\n",
    "![](img_81.png)\n",
    "\n",
    "Por último, vamos a obtener la lista de campos que llevaremos a la dim_clientes, en esta etapa es importante configurar \"Format\", \"length\", \"Precision\" y los demás parámetros antes de la creación del csv. El orden de los campos también debe ser el mismo.\n",
    "\n",
    "![](img_82.png)\n",
    "\n",
    "Ejecutamos ...\n",
    "\n",
    "![](img_83.png)\n",
    "\n",
    "Ahora llevamos el archivo a la tabla dim_cliente con el comando copy. Creamos una nueva transformación en Pentaho con \"Excecute SQL script\".\n",
    "\n",
    "![](img_84.png)\n",
    "\n",
    "A continuación se ve correctamente el script de un copy a Redshift.\n",
    "\n",
    "![](img_85.png)\n",
    "\n",
    "Como ya tenemos el pipeline y la carga desde el csv hacia Reshift, solo nos queda orquestarlo, y esto lo haremos con los \"jobs\" en el mismo Pentaho."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "985bd56c",
   "metadata": {},
   "source": [
    "### <a name=\"mark_24\"></a>Soluciones ETL de las tablas de dimensiones y hechos\n",
    "\n",
    "### [Index](#index_01)\n",
    "\n",
    "Veamos las soluciones para las demás tablas de dimensionales.\n",
    "\n",
    "### dim_territorio:\n",
    "\n",
    "Lo único que cambia en este flujo es la forma en que se insertan los datos, como la tabla de territorio es muy pequeña, Redshift no tiene problemas en realizar un insert directamente.\n",
    "\n",
    "![](img_86.png)\n",
    "\n",
    "![](img_87.png)\n",
    "\n",
    "![](img_88.png)\n",
    "\n",
    "![](img_89.png)\n",
    "\n",
    "### dim_productos:\n",
    "\n",
    "![](img_90.png)\n",
    "\n",
    "### dim_vendedores:\n",
    "\n",
    "![](img_91.png)\n",
    "\n",
    "![](img_92.png)\n",
    "\n",
    "![](img_93.png)\n",
    "\n",
    "![](img_94.png)\n",
    "\n",
    "El año que tenga el 9999 va a ser el registro vigente.\n",
    "\n",
    "![](img_95.png)\n",
    "\n",
    "### dim_tiempo:\n",
    "\n",
    "![](img_96.png)\n",
    "\n",
    "![](img_97.png)\n",
    "\n",
    "![](img_98.png)\n",
    "\n",
    "![](img_99.png)\n",
    "\n",
    "![](img_100.png)\n",
    "\n",
    "### tabla de hechos:\n",
    "\n",
    "![](img_101.png)\n",
    "\n",
    "![](img_102.png)\n",
    "\n",
    "![](img_103.png)\n",
    "\n",
    "Para \"vendedores\".\n",
    "\n",
    "![](img_104.png)\n",
    "\n",
    "para \"input_fechas\":\n",
    "\n",
    "![](img_105.png)\n",
    "\n",
    "![](img_106.png)\n",
    "\n",
    "![](img_107.png)\n",
    "\n",
    "![](img_108.png)\n",
    "\n",
    "![](img_109.png)\n",
    "\n",
    "![](img_110.png)\n",
    "\n",
    "![](img_111.png)\n",
    "\n",
    "![](img_112.png)\n",
    "\n",
    "![](img_113.png)\n",
    "\n",
    "![](img_114.png)\n",
    "\n",
    "![](img_115.png)\n",
    "\n",
    "![](img_116.png)\n",
    "\n",
    "![](img_117.png)\n",
    "\n",
    "![](img_118.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4693bc77",
   "metadata": {},
   "source": [
    "### <a name=\"mark_25\"></a>Orquestación algunos parámetros extra.\n",
    "\n",
    "### [Index](#index_01)\n",
    "\n",
    "Como tenemos una variable en cada una de las transformaciones para calcular un máximo de registro, antes de iniciar con el job realizaremos una transformación más, que se va a encargar de calcular ese máximo, va ir a cada una de las tablas y nos devolverá el máx id de cada una de ellas.\n",
    "\n",
    "Cuando esta transoformación se ejecute solicitará esos 2 parámetros creados.\n",
    "\n",
    "![](img_120.png)\n",
    "\n",
    "![](img_121.png)\n",
    "\n",
    "Ha esa constante le sumamos un registro/valor, si llega el valor de max_id 500 le sumaremos un 1 para que inicie desde el 501\n",
    "\n",
    "![](img_122.png)\n",
    "\n",
    "![](img_123.png)\n",
    "\n",
    "![](img_124.png)\n",
    "\n",
    "![](img_125.png)\n",
    "\n",
    "Ahora haremos lo mismo para calcular una fecha máxima, que se utilizará solo para registrar los registros vigentes, porque solo quiero cargar lo nuevo.\n",
    "\n",
    "![](img_126.png)\n",
    "\n",
    "![](img_127.png)\n",
    "\n",
    "También hay que setear esta nueva variable en todos los inputs de todas las transformaciones. Para solo consultar la data nueva.\n",
    "\n",
    "![](img_128.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137ee680",
   "metadata": {},
   "source": [
    "### <a name=\"mark_26\"></a>Orquestar ETL en Pentaho: job\n",
    "\n",
    "### [Index](#index_01)\n",
    "\n",
    "\n",
    "Creamos un \"job\", primero cargamos todas las dimensiones, esto asegura que al momento de cargar la tabla de hechos ya existan las relaciones necesarias listas.\n",
    "\n",
    "![](img_119.png)\n",
    "\n",
    "Creamos un paso Dummy, el cual nos permitirá llevar todo el proceso en paralelo, primero cargaremos las dimensiones y como no depende unas de otras podemos cargarlas a la vez, y un paso \"wait\" para darle un espacio entre una ejecución y otra, luego conectamos con la primer tran_cliente.\n",
    "\n",
    "![](img_120.png)\n",
    "\n",
    "![](img_121.png)\n",
    "\n",
    "![](img_122.png)\n",
    "\n",
    "![](img_123.png)\n",
    "\n",
    "![](img_124.png)\n",
    "\n",
    "![](img_125.png)\n",
    "\n",
    "![](img_126.png)\n",
    "\n",
    "![](img_127.png)\n",
    "\n",
    "![](img_128.png)\n",
    "\n",
    "![](img_129.png)\n",
    "\n",
    "![](img_130.png)\n",
    "\n",
    "![](img_131.png)\n",
    "\n",
    "![](img_132.png)\n",
    "\n",
    "Agregamos 2 pasos extra en el medio>\n",
    "\n",
    "![](img_133.png)\n",
    "\n",
    "![](img_134.png)\n",
    "\n",
    "![](img_135.png)\n",
    "\n",
    "![](img_136.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59dd949",
   "metadata": {},
   "source": [
    "### <a name=\"mark_27\"></a>Proyecto ETL completo.\n",
    "\n",
    "### [Index](#index_01)\n",
    "\n",
    "https://github.com/platzi/curso-data-warehouse-olap/tree/main/Proyecto%20Data%20Warehouse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_kernel_01",
   "language": "python",
   "name": "data_trans_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
