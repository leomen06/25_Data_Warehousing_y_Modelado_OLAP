{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08040968",
   "metadata": {},
   "source": [
    "### <a name=\"index\"></a>Index\n",
    "\n",
    "[Algunos Conceptos Generales](#mark_01)\n",
    "\n",
    "[OLTP vs. OLAP](#mark_02)\n",
    "\n",
    "[Metodolog칤as de Data Warehouse](#mark_03)\n",
    "\n",
    "   - [Bill Inmon](#mark_04)\n",
    "\n",
    "   - [Ralph Kimball](#mark_05)\n",
    "\n",
    "   - [Hefesto](#mark_06)\n",
    "\n",
    "[Data Warehouse - Data Lake - Data Lakehouse differencias](#mark_07)\n",
    "\n",
    "[Tipos de esquemas dimensionales](#mark_08)\n",
    "\n",
    "[Dimensiones lentamente cambiantes](#mark_09)\n",
    "\n",
    "   - [SCD tipo 1 \"Sobreescribir\"](#mark_10)\n",
    "\n",
    "   - [SCD tipo_2 \"A침adir fila\"](#mark_11)\n",
    "\n",
    "   - [SCD tipo_3 \"A침adir columna\"](#mark_12)\n",
    "\n",
    "   - [SCD tipo_4 \"Historial Separado\"](#mark_13)\n",
    "\n",
    "   - [SCD tipo_6 \"H칤brido\"](#mark_14)\n",
    "\n",
    "[Modelado dimensional: dise침o de modelo](#mark_15)\n",
    "\n",
    "[Configuraci칩n de base de datos y herramientas para el flujo ETL.](#mark_16)\n",
    "\n",
    "[Documentaci칩n Dimensiones y Tabla Fact](#mark_17)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c2f867",
   "metadata": {},
   "source": [
    "# <a name=\"mark_01\"></a>Algunos Conceptos Generales: \n",
    "### [Index](#index)\n",
    "\n",
    "- Anal칤tica Descriptiva: Examina datos hist칩ricos para entender eventos pasados y su impacto en el negocio.\n",
    "\n",
    "- Anal칤tica Diagn칩stica: Investiga las causas de eventos espec칤ficos, identificando patrones y relaciones.\n",
    "\n",
    "- Anal칤tica Predictiva: Utiliza modelos estad칤sticos y algoritmos para prever futuros eventos y tendencias.\n",
    "\n",
    "- Anal칤tica Prescriptiva: Proporciona recomendaciones basadas en an치lisis previos para optimizar decisiones y acciones futuras.\n",
    "\n",
    "- Data Warehouse: Almacen\n",
    "\n",
    "- Data Mart: 츼reas del Almacen\n",
    "\n",
    "- Dimensi칩n: An치lisis de una m칠trica desde distintas perspectivas\n",
    "\n",
    "- Hechos: Informaci칩n cuantitativa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d941a5b",
   "metadata": {},
   "source": [
    "# <a name=\"mark_02\"></a>OLTP vs. OLAP:\n",
    "### [Index](#index)\n",
    "\n",
    "- OLTP (Procesamiento de Transacciones en L칤nea): \n",
    "    - Sistema enfocado en gestionar transacciones r치pidas y eficientes, como ventas o pagos, en tiempo real.\n",
    "    - rapidez, eficiencia, operaciones en tiempo real.\n",
    "\n",
    "- OLAP (Procesamiento Anal칤tico en L칤nea): \n",
    "    - Tecnolog칤a para analizar y consultar grandes vol칰menes de datos multidimensionales, facilitando la toma de decisiones empresariales.\n",
    "    - an치lisis, consulta, datos multidimensionales, toma de decisiones.\n",
    "![](img_01.png)\n",
    "![](img_02.png)\n",
    "Nota: \"Data volatil\" se refiere a las modificaciones que pueden hacerce en un usuario, direcci칩n, email, etc. Informaci칩n es actualizada al momento sin historial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870bd8de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9009dab2",
   "metadata": {},
   "source": [
    "# <a name=\"mark_03\"></a>Metodolog칤as de Data Warehouse:\n",
    "### [Index](#index)\n",
    "\n",
    "* Una m칠todolog칤a orientada a un Data Warehouse contiene b치sicamente 3 componentes:\n",
    "    - Un Source.\n",
    "    - Modelos de Dimensiones (Dimensional Models).\n",
    "    - Su Visualizaci칩n.\n",
    "## Nota_01: Pueden cohexistir m치s de una metodolog칤a por proyecto.\n",
    "## Nota_02: Las Metodolog칤as de DW no son fijas, pueden customizarce dependiendo las necesidades."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52388743",
   "metadata": {},
   "source": [
    "# <a name=\"mark_04\"></a>Bill Inmon:\n",
    "### [Index](#index)\n",
    "\n",
    "Precursor de las tecnolog칤as de BI.\n",
    "\n",
    "1. En este modelo \"Staging\" es una DDBB temporal.\n",
    "    - Podemos hacer transformaciones muy pesadas en los datos sin afectar las DDBB del negocio.\n",
    "2. Proceso de ETL --> Data Warehouse.\n",
    "     \n",
    "![](img_03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8163df5e",
   "metadata": {},
   "source": [
    "# <a name=\"mark_05\"></a>Ralph Kimball:\n",
    "### [Index](#index)\n",
    "\n",
    "![](img_04.png)\n",
    "\n",
    "## Flujo para crear Modelo de Datos:\n",
    "![](img_05.png)\n",
    "\n",
    "Notas:\n",
    "- \"Planificaci칩n del Proyecto\" --> BI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219cd7d2",
   "metadata": {},
   "source": [
    "# <a name=\"mark_06\"></a>Hefesto:\n",
    "### [Index](#index)\n",
    "\n",
    "![](img_06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8424eb",
   "metadata": {},
   "source": [
    "# <a name=\"mark_07\"></a>Data Warehouse - Data Lake - Data Lakehouse differencias:\n",
    "### [Index](#index)\n",
    "\n",
    "* 쯈u칠 es un Data Lakehouse?\n",
    "\n",
    "Un Data Lakehouse es una arquitectura que combina la escalabilidad y rapidez de los Data Lakes con la eficiencia y confiabilidad de los Data Warehouses. \n",
    "\n",
    "Esta arquitectura se basa en un sistema de almacenamiento de datos distribuido, que permite la ingesta y procesamiento de grandes vol칰menes de datos, y una capa de procesamiento que permite la transformaci칩n, integraci칩n y an치lisis de estos datos.\n",
    "\n",
    "A diferencia de los Data Lakes tradicionales, que se basan en almacenar datos sin procesar en su forma original, los Data Lakehouses organizan los datos en un esquema de tablas o columnas para mejorar la eficiencia en la consulta y en el an치lisis de datos. \n",
    "\n",
    "Por otro lado, a diferencia de los Data Warehouses tradicionales, los Data Lakehouses no requieren la definici칩n previa de esquemas de datos y favorecen la escalabilidad horizontal, lo que facilita el procesamiento de grandes vol칰menes de datos.\n",
    "\n",
    "* Diferencias entre un Data Warehouse, un Data Lake y un Data Lakehouse\n",
    "\n",
    "Resulta muy f치cil confundir t칠rminos como Data Warehouse, Data Lake y Data Lakehouse. Cada uno de ellos se refiere a una arquitectura de almacenamiento de datos, pero tienen diferencias significativas.\n",
    "\n",
    "Un Data Warehouse es un repositorio centralizado de datos estructurados, orientado a las consultas y el an치lisis de negocios. \n",
    "![](img_07.png)\n",
    "La informaci칩n se almacena en tablas relacionales y se estructura para facilitar la consulta y el an치lisis. \n",
    "Te recomiendo que leas el [art칤culo espec칤fico de Data Warehouse](https://aprenderbigdata.com/data-warehouse/) para tener m치s detalle.\n",
    "![](https://aprenderbigdata.com/wp-content/uploads/lakehouse-warehouse-datalake.png)\n",
    "Data Warehouse vs Data Lake vs Data Lakehouse [Databricks]\n",
    "\n",
    "Por otro lado, un Data Lake es un repositorio centralizado de datos sin estructurar y semiestructurados. \n",
    "![](img_08.png)\n",
    "Es una soluci칩n m치s flexible y escalable que permite el almacenamiento de datos en bruto sin procesar, lo que permite un an치lisis m치s exhaustivo y una exploraci칩n m치s profunda de los datos. \n",
    "\n",
    "Aqu칤 tienes el [art칤culo en detalle sobre Data Lakes](https://aprenderbigdata.com/data-lake/).\n",
    "\n",
    "Un Data Lakehouse es una combinaci칩n de ambos. Es una soluci칩n h칤brida que combina los beneficios de un Data Warehouse y de un Data Lake. \n",
    "\n",
    "Permite el almacenamiento y procesamiento de datos estructurados, semiestructurados y no estructurados, lo que significa que los datos pueden ser analizados y procesados seg칰n sea necesario. \n",
    "\n",
    "Adem치s, los datos pueden ser utilizados tanto para anal칤tica y para [machine learning](https://aprenderbigdata.com/machine-learning/).\n",
    "\n",
    "* Arquitectura de un Data Lakehouse\n",
    "\n",
    "     - La arquitectura de un Data Lakehouse por tanto combina elementos de un Data Warehouse y un Data Lake. \n",
    "\n",
    "     - Un Data Lakehouse se compone de un almacenamiento escalable y eficiente de datos en bruto, sin procesar. \n",
    "\n",
    "     - Tambi칠n, consta de una capa de procesamiento de datos flexible y escalable y de una capa de servicios para proporcionar un acceso controlado a los datos.\n",
    "\n",
    "La capa de almacenamiento contiene los datos en su formato nativo y est치n organizados por temas o dominios de negocio. \n",
    "\n",
    "El almacenamiento de datos se basa en una infraestructura de almacenamiento distribuida que puede escalar horizontalmente a medida que aumenta la cantidad de datos. Generalmente, ser치n servicios como ADLS en Azure y S3 en AWS.\n",
    "\n",
    "La capa de procesamiento de datos proporciona la capacidad de procesar grandes cantidades de datos de forma paralela y distribuida. \n",
    "\n",
    "Esta capa utiliza tecnolog칤as como Apache Spark o Apache Flink para procesar los datos y transformarlos en informaci칩n valiosa. \n",
    "\n",
    "La capa de procesamiento de datos se ejecuta en un cl칰ster de procesamiento distribuido y escalable que puede ajustarse en funci칩n de la carga de trabajo, por ejemplo usando Databricks.\n",
    "\n",
    "Por 칰ltimo, la capa de servicios proporciona una capa de abstracci칩n que permite a los usuarios acceder a los datos de forma segura y controlada. \n",
    "\n",
    "Esta capa incluye tecnolog칤as como Apache Hive o Presto para permitir el acceso a los datos mediante SQL, y tecnolog칤as de virtualizaci칩n de datos para proporcionar acceso a los datos a trav칠s de APIs RESTful.\n",
    "\n",
    "* Ventajas de utilizar un Data Lakehouse en tu empresa\n",
    "\n",
    "Un Data Lakehouse ofrece varias ventajas t칠cnicas y empresariales sobre las soluciones tradicionales de almacenamiento y procesamiento de datos. A continuaci칩n tienes un listado con algunas de las ventajas m치s importantes:\n",
    "\n",
    "- Flexibilidad: Al combinar caracter칤sticas de un data lake y un data warehouse, ofrece una arquitectura altamente flexible. Puedes almacenar datos en su formato nativo sin tener que preocuparte por el esquema de datos, lo que facilita la integraci칩n de datos de diferentes fuentes. Adem치s, puedes transformar, procesar y consultar los datos en tiempo real sin tener que moverlos a otra ubicaci칩n.\n",
    "- Escalabilidad: Un data lakehouse te permite escalar verticalmente y horizontalmente seg칰n tus necesidades. Puedes agregar m치s recursos de almacenamiento y procesamiento para manejar mayores vol칰menes de datos y consultas m치s complejas. Adem치s, puedes agregar nuevas fuentes de datos sin preocuparte por la capacidad de almacenamiento.\n",
    "- Eficiencia: Al evitar la necesidad de mover datos entre diferentes sistemas de almacenamiento y procesamiento, un data lakehouse puede reducir significativamente los tiempos de procesamiento y aumentar la eficiencia. Tambi칠n puedes utilizar motores de procesamiento distribuido, como Apache Spark, para procesar grandes vol칰menes de datos en paralelo y acelerar el tiempo de procesamiento.\n",
    "- Coste: Al evitar la necesidad de mover datos entre diferentes sistemas de almacenamiento y procesamiento, un data lakehouse puede reducir significativamente el coste de almacenamiento y procesamiento.\n",
    "\n",
    "* Herramientas y tecnolog칤as para implementar un Data Lakehouse\n",
    "\n",
    "Para implementar un Data Lakehouse, se necesita un conjunto de tecnolog칤as y herramientas que permitan integrar, procesar y analizar los datos.\n",
    "\n",
    "Data Lakehouse IA\n",
    "Las opciones de almacenamiento para un Data Lakehouse pueden variar desde sistemas de almacenamiento de archivos distribuidos como HDFS o Amazon S3 hasta bases de datos columnares como Snowflake.\n",
    "\n",
    "Por otro lado, Spark es una tecnolog칤a ampliamente utilizada para procesamiento de datos en tiempo real y batch en un Data Lakehouse. \n",
    "\n",
    "Otras herramientas populares incluyen Apache Flink para streaming y Apache Beam.\n",
    "\n",
    "Tambi칠n necesitaremos herramientas de orquestaci칩n de flujo de trabajo e integraci칩n de datos. Algunas opciones son Apache Airflow, Apache Nifi y Apache Kafka.\n",
    "\n",
    "La elecci칩n de las herramientas y tecnolog칤as adecuadas depender치 de las necesidades espec칤ficas de la empresa y de los datos que se est칠n procesando. \n",
    "\n",
    "Es muy importante conocer estas tecnolog칤as para dise침ar una soluci칩n adecuada y adaptada a estas necesidades.\n",
    "\n",
    "* Desaf칤os y consideraciones a tener en cuenta\n",
    "\n",
    "Aunque un Data Lakehouse puede ser una soluci칩n efectiva para manejar grandes vol칰menes de datos y ofrecer una arquitectura m치s flexible y escalable, tambi칠n hay ciertos desaf칤os y consideraciones importantes que debes tener en cuenta.\n",
    "\n",
    "Al igual que con cualquier soluci칩n de Big Data, es fundamental implementar medidas de seguridad adecuadas para proteger los datos sensibles y garantizar el cumplimiento normativo. \n",
    "\n",
    "Esto puede incluir la implementaci칩n de controles de acceso, encriptaci칩n y monitorizaci칩n.\n",
    "\n",
    "Un Data Lakehouse puede generar grandes vol칰menes de datos de diferentes fuentes y formatos. \n",
    "\n",
    "Es importante contar con herramientas y procesos para administrar y catalogar estos datos de manera efectiva. \n",
    "\n",
    "Debemos definir de pol칤ticas de datos, estandarizar los formatos usados y establecer mecanismos de housekeeping para eliminar datos obsoletos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358de9f",
   "metadata": {},
   "source": [
    "# <a name=\"mark_08\"></a>Tipos de esquemas dimensionales:\n",
    "### [Index](#index)\n",
    "\n",
    "![](img_09.png)\n",
    "\n",
    "![](img_10.png)\n",
    "\n",
    "![](img_11.png)\n",
    "\n",
    "![](img_12.png)\n",
    "\n",
    "![](img_13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b7bd07",
   "metadata": {},
   "source": [
    "# <a name=\"mark_09\"></a>Dimensiones lentamente cambiantes:\n",
    "### [Index](#index)\n",
    "\n",
    "![](img_14.png)\n",
    "\n",
    "![](img_15.png)\n",
    "\n",
    "![](img_16.png)\n",
    "\n",
    "![](img_17.png)\n",
    "\n",
    "Las Dimensiones lentamente cambiantes o SCD (Slowly Changing Dimensions) son Dimensiones en las cuales sus datos tienden a modificarse a trav칠s del tiempo, ya sea de forma ocasional o constante. \n",
    "\n",
    "Cuando ocurren estos cambios, se puede optar por seguir una de estas dos opciones:\n",
    "\n",
    "Registrar el historial de cambios.\n",
    "Reemplazar los valores que sean necesarios.\n",
    "Inicialmente Ralph Kimball plante칩 tres estrategias a seguir cuando se tratan las SCD: tipo 1, tipo 2 y tipo 3; pero a trav칠s de los a침os la comunidad de personas que se encargaba de modelar bases de datos profundiz칩 las definiciones iniciales e incluy칩 varios tipos SCD m치s, por ejemplo: tipo 4 y tipo 6.\n",
    "\n",
    "A continuaci칩n se detallar치 cada tipo de estrategia SCD:\n",
    "\n",
    "- SCD Tipo 1: Sobreescribir.\n",
    "- SCD Tipo 2: A침adir fila.\n",
    "- SCD Tipo 3: A침adir columna.\n",
    "- SCD Tipo 4: Historial separado.\n",
    "- SCD Tipo 6: H칤brido.\n",
    "\n",
    "Cabe destacar que existe un SCD Tipo 0, que representa el NO tener en cuenta los cambios que pudieran llegar a suceder en los datos de las Dimensiones y por consiguiente NO tomar medidas.\n",
    "\n",
    "De acuerdo a la naturaleza del cambio se debe seleccionar qu칠 t칠cnica SCD se utilizar치;  en algunos casos resultar치 conveniente combinar varias t칠cnicas.\n",
    "\n",
    "Es importante se침alar que si bien hay diferentes maneras de implementar cada t칠cnica, es indispensable contar con claves subrogadas en las tablas de Dimensiones para  poder aplicar dichas t칠cnicas.\n",
    "\n",
    "Al aplicar las diferentes t칠cnicas SCD, en muchos casos se deber치 modificar la estructura de la tabla de Dimensi칩n con la que se est칠 trabajando, por lo cual estas modificaciones son recomendables hacerlas al momento de modelar la tabla; aunque tambi칠n pueden hacerse una vez que ya se ha modelado y contiene datos;  as칤 por ejemplo al a침adir una nueva columna se deber치n especificar los valores por defecto que adoptar치n los registros de la tabla."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85253781",
   "metadata": {},
   "source": [
    "# <a name=\"mark_10\"></a>SCD tipo 1 \"Sobreescribir\":\n",
    "### [Index](#index)\n",
    "\n",
    "\n",
    "Este tipo es el m치s b치sico y sencillo de implementar, ya que si bien NO almacena los cambios hist칩ricos, tampoco requiere ning칰n modelado especial y NO necesita que se a침adan nuevos registros a la tabla.\n",
    "\n",
    "En este caso cuando un registro presenta un cambio en alguno de los valores de sus campos, se debe proceder simplemente a actualizar el dato en cuesti칩n, sobreescribiendo el antiguo.\n",
    "\n",
    "Para ejemplificar este caso, se tomar치 como referencia la siguiente tabla:\n",
    "\n",
    "![](img_18.png)\n",
    "\n",
    "Ahora, se supondr치 que este producto ha cambiado de rubro, y ahora ha pasado a ser Rubro 2, entonces se obtendr치 lo siguiente:\n",
    "\n",
    "![](img_19.png)\n",
    "        \n",
    "Usualmente este tipo es utilizado en casos en donde la informaci칩n hist칩rica no sea importante de mantener, tal como sucede cuando se debe modificar el valor de un registro porque tiene errores de ortograf칤a.\n",
    "\n",
    "El ejemplo planteado es solo a fines pr치cticos, ya que con esta t칠cnica, todos los movimientos realizados de Producto 1, que antes pertenec칤an al Rubro 1, ahora pasar치n a ser del Rubro 2, lo cual crear치 una gran inconsistencia en el DW."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a9502",
   "metadata": {},
   "source": [
    "# <a name=\"mark_11\"></a>SCD tipo 2 \"A침adir fila\":\n",
    "### [Index](#index)\n",
    "\n",
    "Esta estrategia requiere que se agreguen algunas columnas adicionales a la tabla de Dimensi칩n, para que almacenen el historial de cambios.\n",
    "\n",
    "Las columnas que suelen agregarse son:\n",
    "\n",
    "    - fechaInicio: fecha desde que entr칩 en vigencia el registro actual. Por defecto suele utilizarse una fecha muy antigua, ejemplo: 01/01/1000.\n",
    "    \n",
    "    - fechaFin: fecha en la cual el registro actual dej칩 de estar en vigencia. Por defecto suele utilizarse una fecha muy futurista, ejemplo: 01/01/9999.\n",
    "\n",
    "    - version: n칰mero secuencial que se incrementa cada nuevo cambio. Por defecto suele comenzar en 1.\n",
    "    \n",
    "    - versionActual: especifica si el campo actual es el vigente. Este valor puede ser en caso de ser verdadero: true o 1; y en caso de ser falso: false o 0.\n",
    "    \n",
    "Entonces, cuando ocurra alg칰n cambio en los valores de los registros, se a침adir치 una nueva fila y se deber치n completar los datos referidos al historial de cambios.\n",
    "\n",
    "Para ejemplificar este caso, se tomar치 como referencia la siguiente tabla:\n",
    "\n",
    "![](img_20.png)\n",
    "A continuaci칩n se a침adir치n las columnas que almacenar치n el historial:\n",
    "\n",
    "![](img_21.png)\n",
    "Ahora, se supondr치 que este producto ha cambiado de Rubro, y ahora a pasado a ser Rubro 2, entonces se obtendr치 lo siguiente:\n",
    "\n",
    "![](img_22.png)\n",
    "Como puede observarse, se lleva a cabo el siguiente proceso:\n",
    "\n",
    "    - Se a침ade una nueva fila con su correspondiente clave subrogada (idProducto).\n",
    "\n",
    "    - Se registra la modificaci칩n (rubro).\n",
    "\n",
    "    - Se actualizan los valores de fechaInicio y fechaFin, tanto de la fila nueva, como la antigua (la que present칩 el cambio).\n",
    "\n",
    "    - Se incrementa en uno el valor del campo version que posee la fila antigua.\n",
    "\n",
    "    - Se actualizan los valores de versionActual, tanto de la fila nueva, como la antigua; dejando a la fila nueva como el registro vigente (true).\n",
    "    \n",
    "Esta t칠cnica permite guardar ilimitada informaci칩n de cambios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bd09fc",
   "metadata": {},
   "source": [
    "# <a name=\"mark_12\"></a>SCD tipo 3 \"A침adir columna\":\n",
    "### [Index](#index)\n",
    "\n",
    "Esta estrategia requiere que se agregue a la tabla de Dimensi칩n una columna adicional por cada columna cuyos valores se desean mantener en un historial de cambios.\n",
    "\n",
    "![](img_20.png)\n",
    "Para mantener el hist칩rico de cambios sobre los datos de la columna rubro se a침adir치 la columna rubroAnterior:\n",
    "\n",
    "![](img_23.png)\n",
    "Ahora, se supondr치 que este producto ha cambiado de rubro, y ahora ha pasado a ser Rubro 2, entonces se obtendr치 lo siguiente:\n",
    "\n",
    "![](img_24.png)\n",
    "Como puede observarse, se lleva a cabo el siguiente proceso:\n",
    "\n",
    "    - En la columna rubroAnterior se coloca el valor antiguo.\n",
    "    - En la columna rubro se coloca el nuevo valor vigente.\n",
    "Esta t칠cnica permite guardar una limitada informaci칩n de cambios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9278c7",
   "metadata": {},
   "source": [
    "# <a name=\"mark_13\"></a>SCD tipo 4 \"Historial separado\":\n",
    "### [Index](#index)\n",
    "\n",
    "\n",
    "Esta t칠cnica se utiliza en combinaci칩n con alguna otra y su funci칩n b치sica es almacenar en una tabla adicional los detalles de cambios hist칩ricos realizados en una tabla de Dimensi칩n.\n",
    "\n",
    "Esta tabla hist칩rica indicar치 por ejemplo qu칠 tipo de operaci칩n se ha realizado (Insert, Update, Delete), sobre qu칠 campo y en qu칠 fecha.\n",
    "\n",
    "El objetivo de mantener esta tabla es el de contar con un detalle de todos los cambios, para luego analizarlos y poder tomar decisiones acerca de cu치l t칠cnica SCD podr칤a aplicarse mejor.\n",
    "\n",
    "Por ejemplo, la siguiente tabla hist칩rica registra los cambios de la tabla de Dimensi칩n dimProductos, la cual supondremos emplea el SCD Tipo 2:\n",
    "\n",
    "![](img_25.png)\n",
    "\n",
    "Tomando como ejemplo el primer registro de esta tabla, la informaci칩n all칤 guardada indica lo siguiente:\n",
    "\n",
    "    - El d칤a 05/06/2000, el registro de la tabla de Dimensi칩n dimProductos con idProducto igual a 1 sufri칩 un cambio de rubro, por lo cual se debi칩 insertar (Insert) una nueva fila con los valores vigentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d399e56d",
   "metadata": {},
   "source": [
    "# <a name=\"mark_14\"></a>SCD tipo 6 \"H칤brido\":\n",
    "### [Index](#index)\n",
    "\n",
    "El SCD Tipo 6 se basa en combinar diferentes t칠cnicas SCD, ellas son:\n",
    "\n",
    "- SCD Tipo 1,\n",
    "- SCD Tipo 2 y\n",
    "- SCD Tipo 3.\n",
    "\n",
    "Y se denomina SCD Tipo 6, simplemente porque:\n",
    "\n",
    "6 = 1 + 2 +3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703f2fd9",
   "metadata": {},
   "source": [
    "# <a name=\"mark_15\"></a>Modelado dimensional: dise침o de modelo\n",
    "### [Index](#index)\n",
    "\n",
    "Utilizando https://dbdiagram.io/d para el modelado de dise침o generamos el siguiente modelo.\n",
    "\n",
    "```sql\n",
    "// Use DBML to define your database structure\n",
    "// Docs: https://dbml.dbdiagram.io/docs\n",
    "\n",
    "table dws.dim_clientes {\n",
    "  id_cliente int pk\n",
    "  codigo_cliente varchar  \n",
    "  nombre varchar\n",
    "  apellido varchar\n",
    "  nombre_apellido varchar\n",
    "  numero_celular varchar\n",
    "  numero_casa varchar\n",
    "  numero_trabajo varchar\n",
    "  ciudad_casa varchar\n",
    "}\n",
    "Ref: dws.dim_clientes.id_cliente < dws.fact_ventas.id_cliente\n",
    "\n",
    "table dws.dim_productos{\n",
    "  id_producto int pk\n",
    "  codigo_producto varchar\n",
    "  nombre_producto varchar\n",
    "  color varchar\n",
    "  tamanio varchar\n",
    "  categoria varchar\n",
    "}\n",
    "Ref: dws.dim_productos.id_producto < dws.fact_ventas.id_producto\n",
    "\n",
    "table dws.fact_ventas{\n",
    "  id_venta int pk\n",
    "  id_cliente int pk\n",
    "  id_producto int pk\n",
    "  cantidad int\n",
    "  valor decimal\n",
    "  descuento decimal\n",
    "  valor_neto decimal\n",
    "}\n",
    "\n",
    "```\n",
    "![](img_26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3be88b5",
   "metadata": {},
   "source": [
    "# <a name=\"mark_16\"></a>Configuraci칩n de base de datos y herramientas para el flujo ETL.\n",
    "### [Index](#index)\n",
    "Fuente: https://platzi.com/clases/7034-data-warehouse/61567-configuracion-de-setup-para-data-warehouse-y-etl/\n",
    "\n",
    "춰Hola, te doy la bienvenida a este tutorial! Configurar치s las bases de datos y herramientas que usaremos para el ETL y crear un data warehouse.\n",
    "\n",
    "Usaremos PostgreSQL con la base de datos Adventureworks. Ser치 nuestra base de datos transaccional y la fuente de informaci칩n para llevar al data warehouse.\n",
    "\n",
    "Ejecuta las siguientes instrucciones para configurar esto:\n",
    "\n",
    "Ruby\n",
    "\n",
    "Instalaci칩n de Ruby en Ubuntu o WSL con Ubuntu\n",
    "1. Abre la terminal de Ubuntu\n",
    "2. Ejecuta el siguiente comando en la terminal para actualizar la lista de paquetes disponibles:\n",
    "```sh\n",
    "sudo apt-get update\n",
    "```\n",
    "3. Una vez actualizada la lista de paquetes, instala Ruby ejecutando el siguiente comando en la terminal:\n",
    "```sh\n",
    "sudo apt-get install ruby-full\n",
    "```\n",
    "4. Verifica que Ruby se haya instalado correctamente ejecutando ruby -v en la terminal.\n",
    "\n",
    "Instalaci칩n de Ruby en Windows\n",
    "\n",
    "1. Descarga el instalador de Ruby desde la p치gina oficial de Ruby para Windows: https://rubyinstaller.org/downloads/\n",
    "2. Selecciona la versi칩n de Ruby que deseas instalar.\n",
    "3. Ejecuta el instalador y sigue las instrucciones del asistente de instalaci칩n.\n",
    "4. Una vez completada la instalaci칩n, abre la l칤nea de comandos de Windows (cmd.exe) y escribe ruby -v para verificar que la instalaci칩n se haya realizado correctamente.\n",
    "\n",
    "Instalaci칩n de Ruby en macOS\n",
    "\n",
    "1. Abre la terminal de macOS.\n",
    "2. Instala Homebrew ejecutando el siguiente comando en la terminal:\n",
    "\n",
    "/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n",
    "\n",
    "3. Una vez instalado Homebrew, ejecuta el siguiente comando en la terminal para instalar Ruby\n",
    "```sh\n",
    "brew install ruby\n",
    "```\n",
    "4. Verifica que Ruby se haya instalado correctamente ejecutando ruby -v en la terminal.\n",
    "Con estos pasos ya has instalado Ruby.\n",
    "\n",
    "PostgreSQL y pgAdmin o DBeaver:\n",
    "\n",
    "Estas herramientas ya deber칤as tenerla instaladas. Si no las tienes, vuelve a revisar esta clase tutorial o sigue la documentaci칩n de PostgreSQL. 拘勇游눠\n",
    "\n",
    "丘멆잺Nota: si usas Windows recuerda asignar las variables de entorno para PostgreSQL.\n",
    "\n",
    "![](img_27.png)\n",
    "\n",
    "Descarga y configuraci칩n de la base de datos AdventureWorks\n",
    "\n",
    "1. Descarga el repositorio en https://github.com/lorint/AdventureWorks-for-Postgres\n",
    "\n",
    "Ejecuta el siguiente comando de Git:\n",
    "\n",
    "git clone https://github.com/lorint/AdventureWorks-for-Postgres.git\n",
    "\n",
    "Este repositorio contiene los archivos para crear las tablas y vistas de la base de datos.\n",
    "\n",
    "2. Descarga Adventure Works 2014 OLTP Script.\n",
    "\n",
    "Contiene los archivos para llenar las tablas de la base de datos.\n",
    "\n",
    "3. Copia y pega el archivo AdventureWorks-oltp-install-script.zip en el directorio AdventureWorks-for-Postgres.\n",
    "\n",
    "4. En tu terminal 칰bicate en el directorio AdventureWorks-for-Postgres y descomprime AdventureWorks-oltp-install-script.zip:\n",
    "```sh\n",
    "cd AdventureWorks-for-Postgres/\n",
    "unzip AdventureWorks-oltp-install-script.zip\n",
    "```\n",
    "5. En la terminal, ubic치ndote en el directorio AdventureWorks-for-Postgres, ejecuta el siguiente comando para convertir los archivos csv:\n",
    "```sh\n",
    "ruby update_csvs.rb\n",
    "```\n",
    "6. Activa la conexi칩n con postgresql:\n",
    "```sh\n",
    "sudo service postgresql start\n",
    "```\n",
    "7. Crea la base de datos con el siguiente comando de PostgreSQL:\n",
    "```sh\n",
    "psql -c \"CREATE DATABASE \\\"Adventureworks\\\";\"\n",
    "```\n",
    "o\n",
    "```sh\n",
    "psql -c \"CREATE DATABASE \\\"Adventureworks\\\";\" -U postgres -h localhost\n",
    "```\n",
    "8. Ejecuta el script que llena las tablas de la base de datos:\n",
    "```sh\n",
    "psql -d Adventureworks < install.sql\n",
    "```\n",
    "o\n",
    "```sh\n",
    "psql -d Adventureworks < install.sql -U postgres -h localhost\n",
    "```\n",
    "# Nota: En este punto hay que hacer una modificaci칩n en el archivo insatll.sql para indicarle donde tiene que ir a buscar los datos para cada archivo CSV, en cada FROM hay que agregar el path correspondiente del archivo, de la siguiente forma.\n",
    "```sql\n",
    "\\copy HumanResources.EmployeeDepartmentHistory FROM 'C:/Users/mende/OneDrive/Documents/Data_Warehousing_y_Modelado_OLAP_Project/AdventureWorks-for-Postgres/EmployeeDepartmentHistory.csv' DELIMITER E'\\t' CSV;\n",
    "SELECT 'Copying data into HumanResources.EmployeePayHistory';\n",
    "```\n",
    "# Luego se ejecuta desde PgAdmn PSQL Tool la siguiente instrucci칩n:\n",
    "```sh\n",
    "\\i C:/Users/mende/OneDrive/Documents/Data_Warehousing_y_Modelado_OLAP_Project/AdventureWorks-for-Postgres/install\n",
    "_02.sql\n",
    "```\n",
    "```\n",
    "# Donde \\i ejecuta archivos. \n",
    "\n",
    "#Para ver los help ejecutar \"\\?\"\n",
    "```\n",
    "9. Conecta tu base de datos en DBeaver o pgAdmin.\n",
    "\n",
    "    1. Abre DBeaver o pgAdmin.\n",
    "\n",
    "    2. Selecciona la opci칩n para crear una nueva conexi칩n.\n",
    "\n",
    "    3. Selecciona PostgreSQL en la lista de bases de datos.\n",
    "\n",
    "    4. Ingresa la informaci칩n de conexi칩n necesaria en la pesta침a.\n",
    "\n",
    "        - Host: localhost\n",
    "        - Port: 5432\n",
    "        - Base de datos: Adventureworks\n",
    "        - Nombre de usuario: postgres\n",
    "        - Password: la que tengas de tu user de postgresql.\n",
    "![](img_28.png)\n",
    "\n",
    "5. Haz clic en **Test Connection** para asegurarte de que los detalles de conexi칩n sean correctos y que puedas conectarte a la base de datos.\n",
    "6. Si la prueba de conexi칩n es exitosa, haz clic en \"Finalizar\" para guardar la configuraci칩n de la conexi칩n.\n",
    "\n",
    "Configuraci칩n de Pentaho\n",
    "\n",
    "Esta herramienta la utilizaremos para crear las ETL de los datos transaccionales (DB Adventureworks) en Postgres a el Data Warehouse en AWS Redshift.\n",
    "\n",
    "Esta herramienta deber칤as tenerla instalada del Curso de Fundamentos de ETL con Python y Pentaho. Si no la tienes revisa esta clase tutorial. 拘勇游눠\n",
    "\n",
    "Instalaci칩n y configuraci칩n de AWS CLI\n",
    "\n",
    "Este servicio lo usar치s para realizar la conexi칩n a S3 y cargar archivos planos que luego ser치n cargados a AWS Redshift con el comando COPY.\n",
    "\n",
    "Esta herramienta la configuraste en el Curso Pr치ctico de AWS: Roles y Seguridad con IAM en su m칩dulo SDK, CLI y AWS Access Keys. 拘勇游눠\n",
    "\n",
    "Vuelve a ver esas clases o sigue la siguiente documentaci칩n de AWS si no lo tienes configurado:\n",
    "\n",
    "Instalar AWS CLI: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html\n",
    "\n",
    "Configurar AWS CLI: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html\n",
    "\n",
    "Configuraci칩n de AWS Redshift\n",
    "\n",
    "AWS Redshift ser치 utilizado como data warehouse. Ser치 el lugar donde construiremos las dimensiones, tablas de hechos y llevaremos los datos modelados y limpios que se obtuvieron del sistema transaccional.\n",
    "\n",
    "1. Crea un nuevo cl칰ster de AWS Redshift de manera similar al Curso de Fundamentos de ETL con Python y Pentaho. Puedes seguir las clases tutoriales de ese curso:\n",
    "\n",
    "    - Configuraci칩n de cl칰ster en AWS Redshift.\n",
    "    \n",
    "丘멆잺 Recuerda nombrar diferente al cl칰ster de AWS Redshift y al bucket de AWS S3 que usar치s para el proyecto de este curso.\n",
    "\n",
    "Con esto has completado la configuraci칩n de herramientas a usar en las siguientes clases del curso.\n",
    "\n",
    "Deja en los comentarios si tienes alguna duda o problema que impida tu progreso, para que en comunidad podamos apoyarte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8165b268",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52763669",
   "metadata": {},
   "source": [
    "# <a name=\"mark_17\"></a>An치lisis Dimensiones y Tabla Fact.\n",
    "### [Index](#index)\n",
    "\n",
    "# Importante: La generaci칩n del modelado ETL ![](img_26.png) es independiente de donde y como se extraen sus datos, a continuaci칩n se realiza el an치lisis para la extracci칩n de cada uno de esas columnas de sus tablas correspondientes.\n",
    "\n",
    "An치lisis \"dim_clientes\":\n",
    "\n",
    "\n",
    "\n",
    "```sql\n",
    "\n",
    "--drop table dim_clientes;\n",
    "create table if not exists dim_clientes \n",
    "(\n",
    "\tid_cliente\tinteger\n",
    "\t,codigo_cliente\tvarchar(20)\n",
    "\t,nombre\tvarchar(50)\n",
    "\t,apellido\tvarchar(50)\n",
    "\t,nombre_completo\tvarchar(100)\n",
    "\t,numero_telefono_celular\tvarchar(20)\n",
    "\t,numero_telefono_casa\tvarchar(20)\n",
    "\t,numero_telefono_trabajo\tvarchar(20)\n",
    "\t,ciudad_casa\tvarchar(50)\n",
    "\t,fecha_carga timestamp\n",
    "\t,fecha_actualizacion timestamp\n",
    "\t,primary key (id_cliente)\n",
    ") \n",
    ";\n",
    "\n",
    "\n",
    "--drop table dim_productos;\n",
    "create table if not exists dim_productos \n",
    "(\n",
    "\tid_producto\tinteger\n",
    "\t,codigo_producto\tvarchar(20)\n",
    "\t,nombre\tvarchar(50)\n",
    "\t,color\tvarchar(50)\n",
    "\t,tamanio\tvarchar(50)\n",
    "\t,categoria\tvarchar(50)\n",
    "\t,fecha_carga timestamp\n",
    "\t,fecha_actualizacion timestamp\n",
    "\t,primary key (id_producto)\n",
    ") \n",
    ";\n",
    "\n",
    "\n",
    "--drop table dim_territorios;\n",
    "create table if not exists dim_territorios\n",
    "(\n",
    "\tid_territorio\tinteger\n",
    "\t,codigo_territorio\tvarchar(20)\n",
    "\t,nombre\tvarchar(50)\n",
    "\t,continente\tvarchar(50)\n",
    "\t,fecha_carga timestamp\n",
    "\t,fecha_actualizacion timestamp\n",
    "\t,primary key (id_territorio)\n",
    ") \n",
    ";\n",
    "\n",
    "\n",
    "--drop table dim_vendedores;\n",
    "create table if not exists dim_vendedores \n",
    "(\n",
    "\tid_vendedor\tinteger\n",
    "\t,codigo_vendedor\tvarchar(20)\n",
    "\t,identificaci칩n\tvarchar(20)\n",
    "\t,nombre\tvarchar(50)\n",
    "\t,apellido\tvarchar(50)\n",
    "\t,nombre_completo\tvarchar(50)\n",
    "\t,rol\tvarchar(50)\n",
    "\t,fecha_nacimiento\tdate\n",
    "\t,genero\tvarchar(10)\n",
    "\t,ind_activo\tboolean\n",
    "\t,fecha_inicio\tdate\n",
    "\t,fecha_fin\tdate\n",
    "\t,version integer\n",
    "\t,fecha_carga timestamp\n",
    "\t,primary key (id_vendedor)\n",
    ") \n",
    ";\n",
    "\n",
    "\n",
    "--drop table fact_ventas;\n",
    "CREATE TABLE if not exists fact_ventas (\n",
    "\tid_venta integer NOT NULL,\n",
    "\tcodigo_venta_detalle varchar(10) NOT NULL,\n",
    "\tcodigo_venta_encabezado varchar(10) NOT NULL,\n",
    "\tid_fecha integer NULL,\n",
    "\tid_territorio integer NULL,\n",
    "\tid_cliente integer NULL,\n",
    "\tid_vendedor integer NULL,\n",
    "\tid_producto integer NULL,\n",
    "\tcantidad integer NULL,\n",
    "\tvalor numeric(18,2) NULL,\n",
    "\tdescuento numeric(18,2) NULL,\n",
    "\tfecha_carga timestamp NULL,\n",
    "\tfecha_actualizacion timestamp NULL,\n",
    "\tCONSTRAINT fact_ventas_pkey PRIMARY KEY (id_venta)\n",
    ")\n",
    "\n",
    "\n",
    "--drop table dim_tiempo;\n",
    "create table if not exists dim_tiempo\n",
    "(\n",
    "    id_fecha int not null,\n",
    "    fecha date not null, \n",
    "    dia smallint not null,\n",
    "    mes smallint not null,\n",
    "    anio smallint not null,\n",
    "    dia_semana smallint not null,\n",
    "    dia_anio smallint not null,\n",
    "\tPRIMARY KEY (id_fecha)\n",
    ")\n",
    "\n",
    "\n",
    "--Ejecutar luego de realizar la primera carga de datos en las dimensiones con Pentaho!!!!\n",
    "\n",
    "INSERT INTO dwh_adventureworks.dim_clientes\n",
    "(id_cliente, codigo_cliente, nombre, apellido, nombre_completo, numero_telefono_celular, numero_telefono_casa, numero_telefono_trabajo, ciudad_casa, fecha_carga, fecha_actualizacion)\n",
    "VALUES(-1, '-1', 'Sin Informaci칩n', 'Sin Informaci칩n', 'Sin Informaci칩n', '', '', '', '', '1900/01/01 00:00:00', '1900/01/01 00:00:00');\n",
    "\n",
    "\n",
    "INSERT INTO dwh_adventureworks.dim_productos\n",
    "(id_producto, codigo_producto, nombre, color, tamanio, categoria, fecha_carga, fecha_actualizacion)\n",
    "VALUES(-1, '-1', 'Sin Informaci칩n', '', '', '', '1900/01/01 00:00:00', '1900/01/01 00:00:00');\n",
    "\n",
    "\n",
    "INSERT INTO dwh_adventureworks.dim_territorios\n",
    "(id_territorio, codigo_territorio, nombre, continente, fecha_carga, fecha_actualizacion)\n",
    "VALUES(-1, '-1', 'Sin Informaci칩n', '', '1900/01/01 00:00:00', '1900/01/01 00:00:00');\n",
    "\n",
    "\n",
    "INSERT INTO dwh_adventureworks.dim_vendedores\n",
    "(id_vendedor, codigo_vendedor, identificaci칩n, nombre, apellido, nombre_completo, rol, fecha_nacimiento, genero, ind_activo, fecha_inicio, fecha_fin, version, fecha_carga)\n",
    "VALUES(-1, '-1', null, 'Sin Informaci칩n', 'Sin Informaci칩n', 'Sin Informaci칩n', null, '1900/01/01 00:00:00', null, true, '1900/01/01 00:00:00', '9999/12/31 00:00:00', 1, '1900/01/01 00:00:00');\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b55659",
   "metadata": {},
   "source": [
    "### SQL\n",
    "\n",
    "```sql\n",
    "select \n",
    "c.customerid as codigo_cliente,\n",
    "p.firstname as nombre,\n",
    "p.lastname as apellido,\n",
    "p.firstname || ' ' ||p.lastname as nombre_apellido,\n",
    "case when ph.phonenumbertypeid = 1 then ph.phonenumber else null end as numero_celular,\n",
    "case when ph.phonenumbertypeid = 2 then ph.phonenumber else null end as numero_casa,\n",
    "case when ph.phonenumbertypeid = 3 then ph.phonenumber else null end as numero_trabajo,\n",
    "ps.name as ciudad_casa\n",
    "\n",
    "from sales.customer as c\n",
    "inner join person.person p\n",
    "on c.personid = p.businessentityid\n",
    "left join person.personphone as ph\n",
    "on ph.businessentityid = c.personid\n",
    "left join person.stateprovince as ps\n",
    "on ps.stateprovinceid = c.territoryid\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d774d069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_kernel_01",
   "language": "python",
   "name": "data_trans_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
