{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08040968",
   "metadata": {},
   "source": [
    "### <a name=\"index\"></a>Index\n",
    "\n",
    "[Algunos Conceptos Generales](#mark_01)\n",
    "\n",
    "[OLTP vs. OLAP](#mark_02)\n",
    "\n",
    "[Metodologías de Data Warehouse](#mark_03)\n",
    "\n",
    "   - [Bill Inmon](#mark_04)\n",
    "\n",
    "   - [Ralph Kimball](#mark_05)\n",
    "\n",
    "   - [Hefesto](#mark_06)\n",
    "\n",
    "[Data Warehouse - Data Lake - Data Lakehouse differencias](#mark_07)\n",
    "\n",
    "[Tipos de esquemas dimensionales](#mark_08)\n",
    "\n",
    "[Dimensiones lentamente cambiantes](#mark_09)\n",
    "\n",
    "   - [SCD tipo 1 \"Sobreescribir\"](#mark_10)\n",
    "\n",
    "   - [SCD tipo_2 \"Añadir fila\"](#mark_11)\n",
    "\n",
    "   - [SCD tipo_3 \"Añadir columna\"](#mark_12)\n",
    "\n",
    "   - [SCD tipo_4 \"Historial Separado\"](#mark_13)\n",
    "\n",
    "   - [SCD tipo_6 \"Híbrido\"](#mark_14)\n",
    "\n",
    "[Modelado dimensional: diseño de modelo](#mark_15)\n",
    "\n",
    "[Configuración de base de datos y herramientas para el flujo ETL.](#mark_16)\n",
    "\n",
    "[Documentación Dimensiones y Tabla Fact](#mark_17)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c2f867",
   "metadata": {},
   "source": [
    "# <a name=\"mark_01\"></a>Algunos Conceptos Generales: \n",
    "### [Index](#index)\n",
    "\n",
    "- Analítica Descriptiva: Examina datos históricos para entender eventos pasados y su impacto en el negocio.\n",
    "\n",
    "- Analítica Diagnóstica: Investiga las causas de eventos específicos, identificando patrones y relaciones.\n",
    "\n",
    "- Analítica Predictiva: Utiliza modelos estadísticos y algoritmos para prever futuros eventos y tendencias.\n",
    "\n",
    "- Analítica Prescriptiva: Proporciona recomendaciones basadas en análisis previos para optimizar decisiones y acciones futuras.\n",
    "\n",
    "- Data Warehouse: Almacen\n",
    "\n",
    "- Data Mart: Áreas del Almacen\n",
    "\n",
    "- Dimensión: Análisis de una métrica desde distintas perspectivas\n",
    "\n",
    "- Hechos: Información cuantitativa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d941a5b",
   "metadata": {},
   "source": [
    "# <a name=\"mark_02\"></a>OLTP vs. OLAP:\n",
    "### [Index](#index)\n",
    "\n",
    "- OLTP (Procesamiento de Transacciones en Línea): \n",
    "    - Sistema enfocado en gestionar transacciones rápidas y eficientes, como ventas o pagos, en tiempo real.\n",
    "    - rapidez, eficiencia, operaciones en tiempo real.\n",
    "\n",
    "- OLAP (Procesamiento Analítico en Línea): \n",
    "    - Tecnología para analizar y consultar grandes volúmenes de datos multidimensionales, facilitando la toma de decisiones empresariales.\n",
    "    - análisis, consulta, datos multidimensionales, toma de decisiones.\n",
    "![](img_01.png)\n",
    "![](img_02.png)\n",
    "Nota: \"Data volatil\" se refiere a las modificaciones que pueden hacerce en un usuario, dirección, email, etc. Información es actualizada al momento sin historial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870bd8de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9009dab2",
   "metadata": {},
   "source": [
    "# <a name=\"mark_03\"></a>Metodologías de Data Warehouse:\n",
    "### [Index](#index)\n",
    "\n",
    "* Una métodología orientada a un Data Warehouse contiene básicamente 3 componentes:\n",
    "    - Un Source.\n",
    "    - Modelos de Dimensiones (Dimensional Models).\n",
    "    - Su Visualización.\n",
    "## Nota_01: Pueden cohexistir más de una metodología por proyecto.\n",
    "## Nota_02: Las Metodologías de DW no son fijas, pueden customizarce dependiendo las necesidades."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52388743",
   "metadata": {},
   "source": [
    "# <a name=\"mark_04\"></a>Bill Inmon:\n",
    "### [Index](#index)\n",
    "\n",
    "Precursor de las tecnologías de BI.\n",
    "\n",
    "1. En este modelo \"Staging\" es una DDBB temporal.\n",
    "    - Podemos hacer transformaciones muy pesadas en los datos sin afectar las DDBB del negocio.\n",
    "2. Proceso de ETL --> Data Warehouse.\n",
    "     \n",
    "![](img_03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8163df5e",
   "metadata": {},
   "source": [
    "# <a name=\"mark_05\"></a>Ralph Kimball:\n",
    "### [Index](#index)\n",
    "\n",
    "![](img_04.png)\n",
    "\n",
    "## Flujo para crear Modelo de Datos:\n",
    "![](img_05.png)\n",
    "\n",
    "Notas:\n",
    "- \"Planificación del Proyecto\" --> BI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219cd7d2",
   "metadata": {},
   "source": [
    "# <a name=\"mark_06\"></a>Hefesto:\n",
    "### [Index](#index)\n",
    "\n",
    "![](img_06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8424eb",
   "metadata": {},
   "source": [
    "# <a name=\"mark_07\"></a>Data Warehouse - Data Lake - Data Lakehouse differencias:\n",
    "### [Index](#index)\n",
    "\n",
    "* ¿Qué es un Data Lakehouse?\n",
    "\n",
    "Un Data Lakehouse es una arquitectura que combina la escalabilidad y rapidez de los Data Lakes con la eficiencia y confiabilidad de los Data Warehouses. \n",
    "\n",
    "Esta arquitectura se basa en un sistema de almacenamiento de datos distribuido, que permite la ingesta y procesamiento de grandes volúmenes de datos, y una capa de procesamiento que permite la transformación, integración y análisis de estos datos.\n",
    "\n",
    "A diferencia de los Data Lakes tradicionales, que se basan en almacenar datos sin procesar en su forma original, los Data Lakehouses organizan los datos en un esquema de tablas o columnas para mejorar la eficiencia en la consulta y en el análisis de datos. \n",
    "\n",
    "Por otro lado, a diferencia de los Data Warehouses tradicionales, los Data Lakehouses no requieren la definición previa de esquemas de datos y favorecen la escalabilidad horizontal, lo que facilita el procesamiento de grandes volúmenes de datos.\n",
    "\n",
    "* Diferencias entre un Data Warehouse, un Data Lake y un Data Lakehouse\n",
    "\n",
    "Resulta muy fácil confundir términos como Data Warehouse, Data Lake y Data Lakehouse. Cada uno de ellos se refiere a una arquitectura de almacenamiento de datos, pero tienen diferencias significativas.\n",
    "\n",
    "Un Data Warehouse es un repositorio centralizado de datos estructurados, orientado a las consultas y el análisis de negocios. \n",
    "![](img_07.png)\n",
    "La información se almacena en tablas relacionales y se estructura para facilitar la consulta y el análisis. \n",
    "Te recomiendo que leas el [artículo específico de Data Warehouse](https://aprenderbigdata.com/data-warehouse/) para tener más detalle.\n",
    "![](https://aprenderbigdata.com/wp-content/uploads/lakehouse-warehouse-datalake.png)\n",
    "Data Warehouse vs Data Lake vs Data Lakehouse [Databricks]\n",
    "\n",
    "Por otro lado, un Data Lake es un repositorio centralizado de datos sin estructurar y semiestructurados. \n",
    "![](img_08.png)\n",
    "Es una solución más flexible y escalable que permite el almacenamiento de datos en bruto sin procesar, lo que permite un análisis más exhaustivo y una exploración más profunda de los datos. \n",
    "\n",
    "Aquí tienes el [artículo en detalle sobre Data Lakes](https://aprenderbigdata.com/data-lake/).\n",
    "\n",
    "Un Data Lakehouse es una combinación de ambos. Es una solución híbrida que combina los beneficios de un Data Warehouse y de un Data Lake. \n",
    "\n",
    "Permite el almacenamiento y procesamiento de datos estructurados, semiestructurados y no estructurados, lo que significa que los datos pueden ser analizados y procesados según sea necesario. \n",
    "\n",
    "Además, los datos pueden ser utilizados tanto para analítica y para [machine learning](https://aprenderbigdata.com/machine-learning/).\n",
    "\n",
    "* Arquitectura de un Data Lakehouse\n",
    "\n",
    "     - La arquitectura de un Data Lakehouse por tanto combina elementos de un Data Warehouse y un Data Lake. \n",
    "\n",
    "     - Un Data Lakehouse se compone de un almacenamiento escalable y eficiente de datos en bruto, sin procesar. \n",
    "\n",
    "     - También, consta de una capa de procesamiento de datos flexible y escalable y de una capa de servicios para proporcionar un acceso controlado a los datos.\n",
    "\n",
    "La capa de almacenamiento contiene los datos en su formato nativo y están organizados por temas o dominios de negocio. \n",
    "\n",
    "El almacenamiento de datos se basa en una infraestructura de almacenamiento distribuida que puede escalar horizontalmente a medida que aumenta la cantidad de datos. Generalmente, serán servicios como ADLS en Azure y S3 en AWS.\n",
    "\n",
    "La capa de procesamiento de datos proporciona la capacidad de procesar grandes cantidades de datos de forma paralela y distribuida. \n",
    "\n",
    "Esta capa utiliza tecnologías como Apache Spark o Apache Flink para procesar los datos y transformarlos en información valiosa. \n",
    "\n",
    "La capa de procesamiento de datos se ejecuta en un clúster de procesamiento distribuido y escalable que puede ajustarse en función de la carga de trabajo, por ejemplo usando Databricks.\n",
    "\n",
    "Por último, la capa de servicios proporciona una capa de abstracción que permite a los usuarios acceder a los datos de forma segura y controlada. \n",
    "\n",
    "Esta capa incluye tecnologías como Apache Hive o Presto para permitir el acceso a los datos mediante SQL, y tecnologías de virtualización de datos para proporcionar acceso a los datos a través de APIs RESTful.\n",
    "\n",
    "* Ventajas de utilizar un Data Lakehouse en tu empresa\n",
    "\n",
    "Un Data Lakehouse ofrece varias ventajas técnicas y empresariales sobre las soluciones tradicionales de almacenamiento y procesamiento de datos. A continuación tienes un listado con algunas de las ventajas más importantes:\n",
    "\n",
    "- Flexibilidad: Al combinar características de un data lake y un data warehouse, ofrece una arquitectura altamente flexible. Puedes almacenar datos en su formato nativo sin tener que preocuparte por el esquema de datos, lo que facilita la integración de datos de diferentes fuentes. Además, puedes transformar, procesar y consultar los datos en tiempo real sin tener que moverlos a otra ubicación.\n",
    "- Escalabilidad: Un data lakehouse te permite escalar verticalmente y horizontalmente según tus necesidades. Puedes agregar más recursos de almacenamiento y procesamiento para manejar mayores volúmenes de datos y consultas más complejas. Además, puedes agregar nuevas fuentes de datos sin preocuparte por la capacidad de almacenamiento.\n",
    "- Eficiencia: Al evitar la necesidad de mover datos entre diferentes sistemas de almacenamiento y procesamiento, un data lakehouse puede reducir significativamente los tiempos de procesamiento y aumentar la eficiencia. También puedes utilizar motores de procesamiento distribuido, como Apache Spark, para procesar grandes volúmenes de datos en paralelo y acelerar el tiempo de procesamiento.\n",
    "- Coste: Al evitar la necesidad de mover datos entre diferentes sistemas de almacenamiento y procesamiento, un data lakehouse puede reducir significativamente el coste de almacenamiento y procesamiento.\n",
    "\n",
    "* Herramientas y tecnologías para implementar un Data Lakehouse\n",
    "\n",
    "Para implementar un Data Lakehouse, se necesita un conjunto de tecnologías y herramientas que permitan integrar, procesar y analizar los datos.\n",
    "\n",
    "Data Lakehouse IA\n",
    "Las opciones de almacenamiento para un Data Lakehouse pueden variar desde sistemas de almacenamiento de archivos distribuidos como HDFS o Amazon S3 hasta bases de datos columnares como Snowflake.\n",
    "\n",
    "Por otro lado, Spark es una tecnología ampliamente utilizada para procesamiento de datos en tiempo real y batch en un Data Lakehouse. \n",
    "\n",
    "Otras herramientas populares incluyen Apache Flink para streaming y Apache Beam.\n",
    "\n",
    "También necesitaremos herramientas de orquestación de flujo de trabajo e integración de datos. Algunas opciones son Apache Airflow, Apache Nifi y Apache Kafka.\n",
    "\n",
    "La elección de las herramientas y tecnologías adecuadas dependerá de las necesidades específicas de la empresa y de los datos que se estén procesando. \n",
    "\n",
    "Es muy importante conocer estas tecnologías para diseñar una solución adecuada y adaptada a estas necesidades.\n",
    "\n",
    "* Desafíos y consideraciones a tener en cuenta\n",
    "\n",
    "Aunque un Data Lakehouse puede ser una solución efectiva para manejar grandes volúmenes de datos y ofrecer una arquitectura más flexible y escalable, también hay ciertos desafíos y consideraciones importantes que debes tener en cuenta.\n",
    "\n",
    "Al igual que con cualquier solución de Big Data, es fundamental implementar medidas de seguridad adecuadas para proteger los datos sensibles y garantizar el cumplimiento normativo. \n",
    "\n",
    "Esto puede incluir la implementación de controles de acceso, encriptación y monitorización.\n",
    "\n",
    "Un Data Lakehouse puede generar grandes volúmenes de datos de diferentes fuentes y formatos. \n",
    "\n",
    "Es importante contar con herramientas y procesos para administrar y catalogar estos datos de manera efectiva. \n",
    "\n",
    "Debemos definir de políticas de datos, estandarizar los formatos usados y establecer mecanismos de housekeeping para eliminar datos obsoletos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358de9f",
   "metadata": {},
   "source": [
    "# <a name=\"mark_08\"></a>Tipos de esquemas dimensionales:\n",
    "### [Index](#index)\n",
    "\n",
    "![](img_09.png)\n",
    "\n",
    "![](img_10.png)\n",
    "\n",
    "![](img_11.png)\n",
    "\n",
    "![](img_12.png)\n",
    "\n",
    "![](img_13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b7bd07",
   "metadata": {},
   "source": [
    "# <a name=\"mark_09\"></a>Dimensiones lentamente cambiantes:\n",
    "### [Index](#index)\n",
    "\n",
    "![](img_14.png)\n",
    "\n",
    "![](img_15.png)\n",
    "\n",
    "![](img_16.png)\n",
    "\n",
    "![](img_17.png)\n",
    "\n",
    "Las Dimensiones lentamente cambiantes o SCD (Slowly Changing Dimensions) son Dimensiones en las cuales sus datos tienden a modificarse a través del tiempo, ya sea de forma ocasional o constante. \n",
    "\n",
    "Cuando ocurren estos cambios, se puede optar por seguir una de estas dos opciones:\n",
    "\n",
    "Registrar el historial de cambios.\n",
    "Reemplazar los valores que sean necesarios.\n",
    "Inicialmente Ralph Kimball planteó tres estrategias a seguir cuando se tratan las SCD: tipo 1, tipo 2 y tipo 3; pero a través de los años la comunidad de personas que se encargaba de modelar bases de datos profundizó las definiciones iniciales e incluyó varios tipos SCD más, por ejemplo: tipo 4 y tipo 6.\n",
    "\n",
    "A continuación se detallará cada tipo de estrategia SCD:\n",
    "\n",
    "- SCD Tipo 1: Sobreescribir.\n",
    "- SCD Tipo 2: Añadir fila.\n",
    "- SCD Tipo 3: Añadir columna.\n",
    "- SCD Tipo 4: Historial separado.\n",
    "- SCD Tipo 6: Híbrido.\n",
    "\n",
    "Cabe destacar que existe un SCD Tipo 0, que representa el NO tener en cuenta los cambios que pudieran llegar a suceder en los datos de las Dimensiones y por consiguiente NO tomar medidas.\n",
    "\n",
    "De acuerdo a la naturaleza del cambio se debe seleccionar qué técnica SCD se utilizará;  en algunos casos resultará conveniente combinar varias técnicas.\n",
    "\n",
    "Es importante señalar que si bien hay diferentes maneras de implementar cada técnica, es indispensable contar con claves subrogadas en las tablas de Dimensiones para  poder aplicar dichas técnicas.\n",
    "\n",
    "Al aplicar las diferentes técnicas SCD, en muchos casos se deberá modificar la estructura de la tabla de Dimensión con la que se esté trabajando, por lo cual estas modificaciones son recomendables hacerlas al momento de modelar la tabla; aunque también pueden hacerse una vez que ya se ha modelado y contiene datos;  así por ejemplo al añadir una nueva columna se deberán especificar los valores por defecto que adoptarán los registros de la tabla."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85253781",
   "metadata": {},
   "source": [
    "# <a name=\"mark_10\"></a>SCD tipo 1 \"Sobreescribir\":\n",
    "### [Index](#index)\n",
    "\n",
    "\n",
    "Este tipo es el más básico y sencillo de implementar, ya que si bien NO almacena los cambios históricos, tampoco requiere ningún modelado especial y NO necesita que se añadan nuevos registros a la tabla.\n",
    "\n",
    "En este caso cuando un registro presenta un cambio en alguno de los valores de sus campos, se debe proceder simplemente a actualizar el dato en cuestión, sobreescribiendo el antiguo.\n",
    "\n",
    "Para ejemplificar este caso, se tomará como referencia la siguiente tabla:\n",
    "\n",
    "![](img_18.png)\n",
    "\n",
    "Ahora, se supondrá que este producto ha cambiado de rubro, y ahora ha pasado a ser Rubro 2, entonces se obtendrá lo siguiente:\n",
    "\n",
    "![](img_19.png)\n",
    "        \n",
    "Usualmente este tipo es utilizado en casos en donde la información histórica no sea importante de mantener, tal como sucede cuando se debe modificar el valor de un registro porque tiene errores de ortografía.\n",
    "\n",
    "El ejemplo planteado es solo a fines prácticos, ya que con esta técnica, todos los movimientos realizados de Producto 1, que antes pertenecían al Rubro 1, ahora pasarán a ser del Rubro 2, lo cual creará una gran inconsistencia en el DW."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a9502",
   "metadata": {},
   "source": [
    "# <a name=\"mark_11\"></a>SCD tipo 2 \"Añadir fila\":\n",
    "### [Index](#index)\n",
    "\n",
    "Esta estrategia requiere que se agreguen algunas columnas adicionales a la tabla de Dimensión, para que almacenen el historial de cambios.\n",
    "\n",
    "Las columnas que suelen agregarse son:\n",
    "\n",
    "    - fechaInicio: fecha desde que entró en vigencia el registro actual. Por defecto suele utilizarse una fecha muy antigua, ejemplo: 01/01/1000.\n",
    "    \n",
    "    - fechaFin: fecha en la cual el registro actual dejó de estar en vigencia. Por defecto suele utilizarse una fecha muy futurista, ejemplo: 01/01/9999.\n",
    "\n",
    "    - version: número secuencial que se incrementa cada nuevo cambio. Por defecto suele comenzar en 1.\n",
    "    \n",
    "    - versionActual: especifica si el campo actual es el vigente. Este valor puede ser en caso de ser verdadero: true o 1; y en caso de ser falso: false o 0.\n",
    "    \n",
    "Entonces, cuando ocurra algún cambio en los valores de los registros, se añadirá una nueva fila y se deberán completar los datos referidos al historial de cambios.\n",
    "\n",
    "Para ejemplificar este caso, se tomará como referencia la siguiente tabla:\n",
    "\n",
    "![](img_20.png)\n",
    "A continuación se añadirán las columnas que almacenarán el historial:\n",
    "\n",
    "![](img_21.png)\n",
    "Ahora, se supondrá que este producto ha cambiado de Rubro, y ahora a pasado a ser Rubro 2, entonces se obtendrá lo siguiente:\n",
    "\n",
    "![](img_22.png)\n",
    "Como puede observarse, se lleva a cabo el siguiente proceso:\n",
    "\n",
    "    - Se añade una nueva fila con su correspondiente clave subrogada (idProducto).\n",
    "\n",
    "    - Se registra la modificación (rubro).\n",
    "\n",
    "    - Se actualizan los valores de fechaInicio y fechaFin, tanto de la fila nueva, como la antigua (la que presentó el cambio).\n",
    "\n",
    "    - Se incrementa en uno el valor del campo version que posee la fila antigua.\n",
    "\n",
    "    - Se actualizan los valores de versionActual, tanto de la fila nueva, como la antigua; dejando a la fila nueva como el registro vigente (true).\n",
    "    \n",
    "Esta técnica permite guardar ilimitada información de cambios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bd09fc",
   "metadata": {},
   "source": [
    "# <a name=\"mark_12\"></a>SCD tipo 3 \"Añadir columna\":\n",
    "### [Index](#index)\n",
    "\n",
    "Esta estrategia requiere que se agregue a la tabla de Dimensión una columna adicional por cada columna cuyos valores se desean mantener en un historial de cambios.\n",
    "\n",
    "![](img_20.png)\n",
    "Para mantener el histórico de cambios sobre los datos de la columna rubro se añadirá la columna rubroAnterior:\n",
    "\n",
    "![](img_23.png)\n",
    "Ahora, se supondrá que este producto ha cambiado de rubro, y ahora ha pasado a ser Rubro 2, entonces se obtendrá lo siguiente:\n",
    "\n",
    "![](img_24.png)\n",
    "Como puede observarse, se lleva a cabo el siguiente proceso:\n",
    "\n",
    "    - En la columna rubroAnterior se coloca el valor antiguo.\n",
    "    - En la columna rubro se coloca el nuevo valor vigente.\n",
    "Esta técnica permite guardar una limitada información de cambios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9278c7",
   "metadata": {},
   "source": [
    "# <a name=\"mark_13\"></a>SCD tipo 4 \"Historial separado\":\n",
    "### [Index](#index)\n",
    "\n",
    "\n",
    "Esta técnica se utiliza en combinación con alguna otra y su función básica es almacenar en una tabla adicional los detalles de cambios históricos realizados en una tabla de Dimensión.\n",
    "\n",
    "Esta tabla histórica indicará por ejemplo qué tipo de operación se ha realizado (Insert, Update, Delete), sobre qué campo y en qué fecha.\n",
    "\n",
    "El objetivo de mantener esta tabla es el de contar con un detalle de todos los cambios, para luego analizarlos y poder tomar decisiones acerca de cuál técnica SCD podría aplicarse mejor.\n",
    "\n",
    "Por ejemplo, la siguiente tabla histórica registra los cambios de la tabla de Dimensión dimProductos, la cual supondremos emplea el SCD Tipo 2:\n",
    "\n",
    "![](img_25.png)\n",
    "\n",
    "Tomando como ejemplo el primer registro de esta tabla, la información allí guardada indica lo siguiente:\n",
    "\n",
    "    - El día 05/06/2000, el registro de la tabla de Dimensión dimProductos con idProducto igual a 1 sufrió un cambio de rubro, por lo cual se debió insertar (Insert) una nueva fila con los valores vigentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d399e56d",
   "metadata": {},
   "source": [
    "# <a name=\"mark_14\"></a>SCD tipo 6 \"Híbrido\":\n",
    "### [Index](#index)\n",
    "\n",
    "El SCD Tipo 6 se basa en combinar diferentes técnicas SCD, ellas son:\n",
    "\n",
    "- SCD Tipo 1,\n",
    "- SCD Tipo 2 y\n",
    "- SCD Tipo 3.\n",
    "\n",
    "Y se denomina SCD Tipo 6, simplemente porque:\n",
    "\n",
    "6 = 1 + 2 +3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703f2fd9",
   "metadata": {},
   "source": [
    "# <a name=\"mark_15\"></a>Modelado dimensional: diseño de modelo\n",
    "### [Index](#index)\n",
    "\n",
    "Utilizando https://dbdiagram.io/d para el modelado de diseño generamos el siguiente modelo.\n",
    "\n",
    "```sql\n",
    "// Use DBML to define your database structure\n",
    "// Docs: https://dbml.dbdiagram.io/docs\n",
    "\n",
    "table dws.dim_clientes {\n",
    "  id_cliente int pk\n",
    "  codigo_cliente varchar  \n",
    "  nombre varchar\n",
    "  apellido varchar\n",
    "  nombre_apellido varchar\n",
    "  numero_celular varchar\n",
    "  numero_casa varchar\n",
    "  numero_trabajo varchar\n",
    "  ciudad_casa varchar\n",
    "}\n",
    "Ref: dws.dim_clientes.id_cliente < dws.fact_ventas.id_cliente\n",
    "\n",
    "table dws.dim_productos{\n",
    "  id_producto int pk\n",
    "  codigo_producto varchar\n",
    "  nombre_producto varchar\n",
    "  color varchar\n",
    "  tamanio varchar\n",
    "  categoria varchar\n",
    "}\n",
    "Ref: dws.dim_productos.id_producto < dws.fact_ventas.id_producto\n",
    "\n",
    "table dws.fact_ventas{\n",
    "  id_venta int pk\n",
    "  id_cliente int pk\n",
    "  id_producto int pk\n",
    "  cantidad int\n",
    "  valor decimal\n",
    "  descuento decimal\n",
    "  valor_neto decimal\n",
    "}\n",
    "\n",
    "```\n",
    "![](img_26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3be88b5",
   "metadata": {},
   "source": [
    "# <a name=\"mark_16\"></a>Configuración de base de datos y herramientas para el flujo ETL.\n",
    "### [Index](#index)\n",
    "Fuente: https://platzi.com/clases/7034-data-warehouse/61567-configuracion-de-setup-para-data-warehouse-y-etl/\n",
    "\n",
    "¡Hola, te doy la bienvenida a este tutorial! Configurarás las bases de datos y herramientas que usaremos para el ETL y crear un data warehouse.\n",
    "\n",
    "Usaremos PostgreSQL con la base de datos Adventureworks. Será nuestra base de datos transaccional y la fuente de información para llevar al data warehouse.\n",
    "\n",
    "Ejecuta las siguientes instrucciones para configurar esto:\n",
    "\n",
    "Ruby\n",
    "\n",
    "Instalación de Ruby en Ubuntu o WSL con Ubuntu\n",
    "1. Abre la terminal de Ubuntu\n",
    "2. Ejecuta el siguiente comando en la terminal para actualizar la lista de paquetes disponibles:\n",
    "```sh\n",
    "sudo apt-get update\n",
    "```\n",
    "3. Una vez actualizada la lista de paquetes, instala Ruby ejecutando el siguiente comando en la terminal:\n",
    "```sh\n",
    "sudo apt-get install ruby-full\n",
    "```\n",
    "4. Verifica que Ruby se haya instalado correctamente ejecutando ruby -v en la terminal.\n",
    "\n",
    "Instalación de Ruby en Windows\n",
    "\n",
    "1. Descarga el instalador de Ruby desde la página oficial de Ruby para Windows: https://rubyinstaller.org/downloads/\n",
    "2. Selecciona la versión de Ruby que deseas instalar.\n",
    "3. Ejecuta el instalador y sigue las instrucciones del asistente de instalación.\n",
    "4. Una vez completada la instalación, abre la línea de comandos de Windows (cmd.exe) y escribe ruby -v para verificar que la instalación se haya realizado correctamente.\n",
    "\n",
    "Instalación de Ruby en macOS\n",
    "\n",
    "1. Abre la terminal de macOS.\n",
    "2. Instala Homebrew ejecutando el siguiente comando en la terminal:\n",
    "\n",
    "/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n",
    "\n",
    "3. Una vez instalado Homebrew, ejecuta el siguiente comando en la terminal para instalar Ruby\n",
    "```sh\n",
    "brew install ruby\n",
    "```\n",
    "4. Verifica que Ruby se haya instalado correctamente ejecutando ruby -v en la terminal.\n",
    "Con estos pasos ya has instalado Ruby.\n",
    "\n",
    "PostgreSQL y pgAdmin o DBeaver:\n",
    "\n",
    "Estas herramientas ya deberías tenerla instaladas. Si no las tienes, vuelve a revisar esta clase tutorial o sigue la documentación de PostgreSQL. ⬅️💡\n",
    "\n",
    "⚠️Nota: si usas Windows recuerda asignar las variables de entorno para PostgreSQL.\n",
    "\n",
    "![](img_27.png)\n",
    "\n",
    "Descarga y configuración de la base de datos AdventureWorks\n",
    "\n",
    "1. Descarga el repositorio en https://github.com/lorint/AdventureWorks-for-Postgres\n",
    "\n",
    "Ejecuta el siguiente comando de Git:\n",
    "\n",
    "git clone https://github.com/lorint/AdventureWorks-for-Postgres.git\n",
    "\n",
    "Este repositorio contiene los archivos para crear las tablas y vistas de la base de datos.\n",
    "\n",
    "2. Descarga Adventure Works 2014 OLTP Script.\n",
    "\n",
    "Contiene los archivos para llenar las tablas de la base de datos.\n",
    "\n",
    "3. Copia y pega el archivo AdventureWorks-oltp-install-script.zip en el directorio AdventureWorks-for-Postgres.\n",
    "\n",
    "4. En tu terminal úbicate en el directorio AdventureWorks-for-Postgres y descomprime AdventureWorks-oltp-install-script.zip:\n",
    "```sh\n",
    "cd AdventureWorks-for-Postgres/\n",
    "unzip AdventureWorks-oltp-install-script.zip\n",
    "```\n",
    "5. En la terminal, ubicándote en el directorio AdventureWorks-for-Postgres, ejecuta el siguiente comando para convertir los archivos csv:\n",
    "```sh\n",
    "ruby update_csvs.rb\n",
    "```\n",
    "6. Activa la conexión con postgresql:\n",
    "```sh\n",
    "sudo service postgresql start\n",
    "```\n",
    "7. Crea la base de datos con el siguiente comando de PostgreSQL:\n",
    "```sh\n",
    "psql -c \"CREATE DATABASE \\\"Adventureworks\\\";\"\n",
    "```\n",
    "o\n",
    "```sh\n",
    "psql -c \"CREATE DATABASE \\\"Adventureworks\\\";\" -U postgres -h localhost\n",
    "```\n",
    "8. Ejecuta el script que llena las tablas de la base de datos:\n",
    "```sh\n",
    "psql -d Adventureworks < install.sql\n",
    "```\n",
    "o\n",
    "```sh\n",
    "psql -d Adventureworks < install.sql -U postgres -h localhost\n",
    "```\n",
    "# Nota: En este punto hay que hacer una modificación en el archivo insatll.sql para indicarle donde tiene que ir a buscar los datos para cada archivo CSV, en cada FROM hay que agregar el path correspondiente del archivo, de la siguiente forma.\n",
    "```sql\n",
    "\\copy HumanResources.EmployeeDepartmentHistory FROM 'C:/Users/mende/OneDrive/Documents/Data_Warehousing_y_Modelado_OLAP_Project/AdventureWorks-for-Postgres/EmployeeDepartmentHistory.csv' DELIMITER E'\\t' CSV;\n",
    "SELECT 'Copying data into HumanResources.EmployeePayHistory';\n",
    "```\n",
    "# Luego se ejecuta desde PgAdmn PSQL Tool la siguiente instrucción:\n",
    "```sh\n",
    "\\i C:/Users/mende/OneDrive/Documents/Data_Warehousing_y_Modelado_OLAP_Project/AdventureWorks-for-Postgres/install\n",
    "_02.sql\n",
    "```\n",
    "```\n",
    "# Donde \\i ejecuta archivos. \n",
    "\n",
    "#Para ver los help ejecutar \"\\?\"\n",
    "```\n",
    "9. Conecta tu base de datos en DBeaver o pgAdmin.\n",
    "\n",
    "    1. Abre DBeaver o pgAdmin.\n",
    "\n",
    "    2. Selecciona la opción para crear una nueva conexión.\n",
    "\n",
    "    3. Selecciona PostgreSQL en la lista de bases de datos.\n",
    "\n",
    "    4. Ingresa la información de conexión necesaria en la pestaña.\n",
    "\n",
    "        - Host: localhost\n",
    "        - Port: 5432\n",
    "        - Base de datos: Adventureworks\n",
    "        - Nombre de usuario: postgres\n",
    "        - Password: la que tengas de tu user de postgresql.\n",
    "![](img_28.png)\n",
    "\n",
    "5. Haz clic en **Test Connection** para asegurarte de que los detalles de conexión sean correctos y que puedas conectarte a la base de datos.\n",
    "6. Si la prueba de conexión es exitosa, haz clic en \"Finalizar\" para guardar la configuración de la conexión.\n",
    "\n",
    "Configuración de Pentaho\n",
    "\n",
    "Esta herramienta la utilizaremos para crear las ETL de los datos transaccionales (DB Adventureworks) en Postgres a el Data Warehouse en AWS Redshift.\n",
    "\n",
    "Esta herramienta deberías tenerla instalada del Curso de Fundamentos de ETL con Python y Pentaho. Si no la tienes revisa esta clase tutorial. ⬅️💡\n",
    "\n",
    "Instalación y configuración de AWS CLI\n",
    "\n",
    "Este servicio lo usarás para realizar la conexión a S3 y cargar archivos planos que luego serán cargados a AWS Redshift con el comando COPY.\n",
    "\n",
    "Esta herramienta la configuraste en el Curso Práctico de AWS: Roles y Seguridad con IAM en su módulo SDK, CLI y AWS Access Keys. ⬅️💡\n",
    "\n",
    "Vuelve a ver esas clases o sigue la siguiente documentación de AWS si no lo tienes configurado:\n",
    "\n",
    "Instalar AWS CLI: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html\n",
    "\n",
    "Configurar AWS CLI: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html\n",
    "\n",
    "Configuración de AWS Redshift\n",
    "\n",
    "AWS Redshift será utilizado como data warehouse. Será el lugar donde construiremos las dimensiones, tablas de hechos y llevaremos los datos modelados y limpios que se obtuvieron del sistema transaccional.\n",
    "\n",
    "1. Crea un nuevo clúster de AWS Redshift de manera similar al Curso de Fundamentos de ETL con Python y Pentaho. Puedes seguir las clases tutoriales de ese curso:\n",
    "\n",
    "    - Configuración de clúster en AWS Redshift.\n",
    "    \n",
    "⚠️ Recuerda nombrar diferente al clúster de AWS Redshift y al bucket de AWS S3 que usarás para el proyecto de este curso.\n",
    "\n",
    "Con esto has completado la configuración de herramientas a usar en las siguientes clases del curso.\n",
    "\n",
    "Deja en los comentarios si tienes alguna duda o problema que impida tu progreso, para que en comunidad podamos apoyarte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8165b268",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52763669",
   "metadata": {},
   "source": [
    "# <a name=\"mark_17\"></a>Análisis Dimensiones y Tabla Fact.\n",
    "### [Index](#index)\n",
    "\n",
    "# Importante: La generación del modelado ETL ![](img_26.png) es independiente de donde y como se extraen sus datos, a continuación se realiza el análisis para la extracción de cada uno de esas columnas de sus tablas correspondientes.\n",
    "\n",
    "Análisis \"dim_clientes\":\n",
    "\n",
    "\n",
    "\n",
    "```sql\n",
    "\n",
    "--drop table dim_clientes;\n",
    "create table if not exists dim_clientes \n",
    "(\n",
    "\tid_cliente\tinteger\n",
    "\t,codigo_cliente\tvarchar(20)\n",
    "\t,nombre\tvarchar(50)\n",
    "\t,apellido\tvarchar(50)\n",
    "\t,nombre_completo\tvarchar(100)\n",
    "\t,numero_telefono_celular\tvarchar(20)\n",
    "\t,numero_telefono_casa\tvarchar(20)\n",
    "\t,numero_telefono_trabajo\tvarchar(20)\n",
    "\t,ciudad_casa\tvarchar(50)\n",
    "\t,fecha_carga timestamp\n",
    "\t,fecha_actualizacion timestamp\n",
    "\t,primary key (id_cliente)\n",
    ") \n",
    ";\n",
    "\n",
    "\n",
    "--drop table dim_productos;\n",
    "create table if not exists dim_productos \n",
    "(\n",
    "\tid_producto\tinteger\n",
    "\t,codigo_producto\tvarchar(20)\n",
    "\t,nombre\tvarchar(50)\n",
    "\t,color\tvarchar(50)\n",
    "\t,tamanio\tvarchar(50)\n",
    "\t,categoria\tvarchar(50)\n",
    "\t,fecha_carga timestamp\n",
    "\t,fecha_actualizacion timestamp\n",
    "\t,primary key (id_producto)\n",
    ") \n",
    ";\n",
    "\n",
    "\n",
    "--drop table dim_territorios;\n",
    "create table if not exists dim_territorios\n",
    "(\n",
    "\tid_territorio\tinteger\n",
    "\t,codigo_territorio\tvarchar(20)\n",
    "\t,nombre\tvarchar(50)\n",
    "\t,continente\tvarchar(50)\n",
    "\t,fecha_carga timestamp\n",
    "\t,fecha_actualizacion timestamp\n",
    "\t,primary key (id_territorio)\n",
    ") \n",
    ";\n",
    "\n",
    "\n",
    "--drop table dim_vendedores;\n",
    "create table if not exists dim_vendedores \n",
    "(\n",
    "\tid_vendedor\tinteger\n",
    "\t,codigo_vendedor\tvarchar(20)\n",
    "\t,identificación\tvarchar(20)\n",
    "\t,nombre\tvarchar(50)\n",
    "\t,apellido\tvarchar(50)\n",
    "\t,nombre_completo\tvarchar(50)\n",
    "\t,rol\tvarchar(50)\n",
    "\t,fecha_nacimiento\tdate\n",
    "\t,genero\tvarchar(10)\n",
    "\t,ind_activo\tboolean\n",
    "\t,fecha_inicio\tdate\n",
    "\t,fecha_fin\tdate\n",
    "\t,version integer\n",
    "\t,fecha_carga timestamp\n",
    "\t,primary key (id_vendedor)\n",
    ") \n",
    ";\n",
    "\n",
    "\n",
    "--drop table fact_ventas;\n",
    "CREATE TABLE if not exists fact_ventas (\n",
    "\tid_venta integer NOT NULL,\n",
    "\tcodigo_venta_detalle varchar(10) NOT NULL,\n",
    "\tcodigo_venta_encabezado varchar(10) NOT NULL,\n",
    "\tid_fecha integer NULL,\n",
    "\tid_territorio integer NULL,\n",
    "\tid_cliente integer NULL,\n",
    "\tid_vendedor integer NULL,\n",
    "\tid_producto integer NULL,\n",
    "\tcantidad integer NULL,\n",
    "\tvalor numeric(18,2) NULL,\n",
    "\tdescuento numeric(18,2) NULL,\n",
    "\tfecha_carga timestamp NULL,\n",
    "\tfecha_actualizacion timestamp NULL,\n",
    "\tCONSTRAINT fact_ventas_pkey PRIMARY KEY (id_venta)\n",
    ")\n",
    "\n",
    "\n",
    "--drop table dim_tiempo;\n",
    "create table if not exists dim_tiempo\n",
    "(\n",
    "    id_fecha int not null,\n",
    "    fecha date not null, \n",
    "    dia smallint not null,\n",
    "    mes smallint not null,\n",
    "    anio smallint not null,\n",
    "    dia_semana smallint not null,\n",
    "    dia_anio smallint not null,\n",
    "\tPRIMARY KEY (id_fecha)\n",
    ")\n",
    "\n",
    "\n",
    "--Ejecutar luego de realizar la primera carga de datos en las dimensiones con Pentaho!!!!\n",
    "\n",
    "INSERT INTO dwh_adventureworks.dim_clientes\n",
    "(id_cliente, codigo_cliente, nombre, apellido, nombre_completo, numero_telefono_celular, numero_telefono_casa, numero_telefono_trabajo, ciudad_casa, fecha_carga, fecha_actualizacion)\n",
    "VALUES(-1, '-1', 'Sin Información', 'Sin Información', 'Sin Información', '', '', '', '', '1900/01/01 00:00:00', '1900/01/01 00:00:00');\n",
    "\n",
    "\n",
    "INSERT INTO dwh_adventureworks.dim_productos\n",
    "(id_producto, codigo_producto, nombre, color, tamanio, categoria, fecha_carga, fecha_actualizacion)\n",
    "VALUES(-1, '-1', 'Sin Información', '', '', '', '1900/01/01 00:00:00', '1900/01/01 00:00:00');\n",
    "\n",
    "\n",
    "INSERT INTO dwh_adventureworks.dim_territorios\n",
    "(id_territorio, codigo_territorio, nombre, continente, fecha_carga, fecha_actualizacion)\n",
    "VALUES(-1, '-1', 'Sin Información', '', '1900/01/01 00:00:00', '1900/01/01 00:00:00');\n",
    "\n",
    "\n",
    "INSERT INTO dwh_adventureworks.dim_vendedores\n",
    "(id_vendedor, codigo_vendedor, identificación, nombre, apellido, nombre_completo, rol, fecha_nacimiento, genero, ind_activo, fecha_inicio, fecha_fin, version, fecha_carga)\n",
    "VALUES(-1, '-1', null, 'Sin Información', 'Sin Información', 'Sin Información', null, '1900/01/01 00:00:00', null, true, '1900/01/01 00:00:00', '9999/12/31 00:00:00', 1, '1900/01/01 00:00:00');\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b55659",
   "metadata": {},
   "source": [
    "### SQL\n",
    "\n",
    "```sql\n",
    "select \n",
    "c.customerid as codigo_cliente,\n",
    "p.firstname as nombre,\n",
    "p.lastname as apellido,\n",
    "p.firstname || ' ' ||p.lastname as nombre_apellido,\n",
    "case when ph.phonenumbertypeid = 1 then ph.phonenumber else null end as numero_celular,\n",
    "case when ph.phonenumbertypeid = 2 then ph.phonenumber else null end as numero_casa,\n",
    "case when ph.phonenumbertypeid = 3 then ph.phonenumber else null end as numero_trabajo,\n",
    "ps.name as ciudad_casa\n",
    "\n",
    "from sales.customer as c\n",
    "inner join person.person p\n",
    "on c.personid = p.businessentityid\n",
    "left join person.personphone as ph\n",
    "on ph.businessentityid = c.personid\n",
    "left join person.stateprovince as ps\n",
    "on ps.stateprovinceid = c.territoryid\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d774d069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_kernel_01",
   "language": "python",
   "name": "data_trans_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
